#!/usr/bin/env python3
"""
Create CIFAR-100-LT splits with BALANCED test sets (8:1:1 ratio).

This replaces the old duplication-based approach with:
- Train: Long-tail (unchanged)
- Test: 80 per class = 8,000 samples (balanced, no duplication)
- Val: 10 per class = 1,000 samples (balanced, no duplication)
- TuneV: 10 per class = 1,000 samples (balanced, no duplication)

NO data leakage, NO duplication.
Reweighting happens at metric computation time.

Usage:
    python create_splits_fixed.py
    python create_splits_fixed.py --imb-factor 100 --output data/my_splits
"""

import argparse
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.data.balanced_test_splits import create_cifar100_lt_balanced_test_splits


def main():
    parser = argparse.ArgumentParser(
        description="Create CIFAR-100-LT with balanced test splits (8:1:1)"
    )
    
    parser.add_argument(
        '--imb-factor',
        type=float,
        default=100,
        help='Imbalance factor for training set (default: 100)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='data/cifar100_lt_if100_splits_fixed',
        help='Output directory (default: data/cifar100_lt_if100_splits_fixed)'
    )
    
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed (default: 42)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Print verbose output'
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("CIFAR-100-LT DATA PREPARATION (BALANCED TEST - NO DUPLICATION)")
    print("="*80)
    print("\nConfiguration:")
    print(f"  ‚Ä¢ Imbalance Factor: {args.imb_factor}")
    print(f"  ‚Ä¢ Output Directory: {args.output}")
    print(f"  ‚Ä¢ Random Seed: {args.seed}")
    print(f"\nSplit Ratios (from CIFAR-100 test set):")
    print(f"  ‚Ä¢ Test:  80% (8,000 samples, 80 per class)")
    print(f"  ‚Ä¢ Val:   10% (1,000 samples, 10 per class)")
    print(f"  ‚Ä¢ TuneV: 10% (1,000 samples, 10 per class)")
    print(f"\n{'='*80}\n")
    
    try:
        # Create splits
        datasets, splits, class_weights = create_cifar100_lt_balanced_test_splits(
            imb_factor=args.imb_factor,
            output_dir=args.output,
            seed=args.seed
        )
        
        print("\n" + "="*80)
        print("‚úÖ SUCCESS!")
        print("="*80)
        
        print("\nüìä Datasets Created:")
        for name, dataset in datasets.items():
            print(f"  {name:>6}: {len(dataset):>6,} samples")
        
        print("\nüìÅ Files Saved to: " + args.output)
        output_path = Path(args.output)
        if output_path.exists():
            for json_file in sorted(output_path.glob("*.json")):
                print(f"  ‚úì {json_file.name}")
        
        print("\n" + "="*80)
        print("NEXT STEPS")
        print("="*80)
        print("\n1Ô∏è‚É£  Train Expert Models:")
        print("   python train_experts.py")
        print("\n2Ô∏è‚É£  Train Gating Network:")
        print("   python train_gating.py --mode pretrain")
        print("   python train_gating.py --mode selective")
        print("\n3Ô∏è‚É£  Train AR-GSE:")
        print("   python train_argse.py")
        print("\n4Ô∏è‚É£  Evaluate with Reweighted Metrics:")
        print("   python evaluate_argse.py")
        print("\n‚ö†Ô∏è  IMPORTANT:")
        print(f"   Use {args.output}/class_weights.json for reweighted metrics!")
        
    except Exception as e:
        print("\n" + "="*80)
        print("‚ùå ERROR")
        print("="*80)
        print(f"\n{str(e)}")
        print("\nPlease check:")
        print("  ‚Ä¢ CIFAR-100 data is accessible")
        print("  ‚Ä¢ Required dependencies are installed (numpy, torchvision)")
        print("  ‚Ä¢ Sufficient disk space")
        print("  ‚Ä¢ Write permissions for output directory")
        sys.exit(1)


if __name__ == "__main__":
    main()
