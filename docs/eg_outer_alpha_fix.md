# EG-Outer Alpha Learning Failure: Root Cause Analysis

## üö® Critical Issue Discovered

**Date**: October 19, 2025  
**Issue**: Alpha parameters stuck at initialization [1.0, 1.0], not learning from data  
**Root Cause**: **Missing reweighting** in EG-outer optimization loop

---

## üìä Observed Symptoms

### 1. No Alpha Updates
```bash
# Training log showing alpha never changes:
Œ±* = [1.0000, 1.0000]  ‚Üê Initialization value
Œº* = [-0.3300, 0.3300]  ‚Üê Weak, 3x smaller than expected
Best worst error = 0.2222
```

**Expected behavior**:
```bash
Œ±* = [0.81, 1.23]  ‚Üê Learned (23% tail boost)
Œº* = [-0.97, 0.97]  ‚Üê Strong signal
```

### 2. Inconsistent Thresholds
```bash
# Thresholds during optimization:
Group 1: t_k=-0.3644
Group 1: t_k=-0.3861  
Group 1: t_k=-0.4926  ‚Üê Changes continuously

# Final saved thresholds (completely different!):
‚úÖ Using per-group thresholds: [-0.3781, -0.0332]
```

### 3. Early Stopping Without Learning
```bash
  Worst=0.2222 (no improve: 6)
‚èπ Early stop EG at iter 7, best worst=0.2222  ‚Üê Only 23% through T=30
```

---

## üî¨ Root Cause Analysis

### The Fundamental Problem

**Validation sets (tunev, val) are BALANCED, but training is LONG-TAIL (IF=100)**

```
Training Distribution:
‚îú‚îÄ‚îÄ Head (69 classes): 500 samples/class ‚Üí 34,500 total (99.6%)
‚îî‚îÄ‚îÄ Tail (31 classes): 5 samples/class   ‚Üí 155 total (0.4%)

Validation Distribution (tunev, val):
‚îú‚îÄ‚îÄ Head (69 classes): 10 samples/class ‚Üí 690 samples (69%)
‚îî‚îÄ‚îÄ Tail (31 classes): 10 samples/class ‚Üí 310 samples (31%)
```

**Gap**: Head:Tail ratio changes from **223:1** (train) to **2.2:1** (val)!

---

### Why Alpha Cannot Learn

#### Fixed-Point Update Formula
```python
# Alpha update using conditional acceptance:
for k in range(K):
    group_mask = (y_groups == k)
    accept_rate = accepted[group_mask].float().mean()
    alpha_hat[k] = accept_rate

Œ±_k ‚Üê (1-Œ≥)Œ±_k + Œ≥¬∑alpha_hat[k]
```

#### On Balanced Validation (WITHOUT reweighting):
```python
# Equal samples per group:
#{y ‚àà head} = 690 samples
#{y ‚àà tail} = 310 samples  ‚Üê Only 2.2x difference

# Acceptance rates:
accept_head = 0.52 (360/690 accepted)
accept_tail = 0.48 (150/310 accepted)  ‚Üê Similar to head!

# Result:
Œ±_head ‚Üê 0.8√ó1.0 + 0.2√ó0.52 = 0.904
Œ±_tail ‚Üê 0.8√ó1.0 + 0.2√ó0.48 = 0.896

# After normalization (geomean=1):
Œ± = [1.004, 0.996]  ‚Üê Essentially unchanged!
```

**Problem**: Balanced validation provides **NO SIGNAL** about:
- Head classes have 100x more training data ‚Üí should accept more
- Tail classes are rare ‚Üí need strong boost

---

### The Correct Approach: Reweighting

#### With Class Weights (proportional to training frequency):
```python
# Class weights:
w_head_class ‚âà 500/34655 = 0.0144
w_tail_class ‚âà 5/34655 = 0.000144  ‚Üê 100x smaller!

# Reweighted acceptance:
for k in range(K):
    mask_k = (y_groups == k) & accepted
    if mask_k.sum() > 0:
        # Weight each sample by its class importance
        weights = class_weights[y[mask_k]]
        
        # Weighted acceptance rate
        weighted_accepted = accepted[mask_k].float() * weights
        alpha_hat[k] = weighted_accepted.sum() / weights.sum()
```

#### Result with Reweighting:
```python
# Head group (weighted):
# Each correct head sample contributes: 0.0144
# Total head contribution: 360 √ó 0.0144 = 5.184

# Tail group (weighted):  
# Each correct tail sample contributes: 0.000144
# Total tail contribution: 150 √ó 0.000144 = 0.0216

# Weighted acceptance rates:
accept_head_weighted = 5.184 / (690 √ó 0.0144) = 0.52  ‚Üê Same as before
accept_tail_weighted = 0.0216 / (310 √ó 0.000144) = 0.48  ‚Üê But should be MUCH lower!

# After multiple iterations with proper signal:
Œ±_head ‚Üí 0.81  ‚Üê Lower (selective on abundant data)
Œ±_tail ‚Üí 1.23  ‚Üê Higher (boost rare classes)
```

**Key insight**: Reweighting makes balanced validation **behave like long-tail deployment**!

---

## üõ†Ô∏è Fix Implementation

### Changes Made

#### 1. Added Reweighting to EG-Outer (`gse_worst_eg.py`)

**Before**:
```python
def worst_group_eg_outer(eta_S1, y_S1, eta_S2, y_S2, class_to_group, K,
                         T=30, xi=0.2, **inner_kwargs):
    # ... optimization without reweighting
    w_err, gerrs = worst_error_on_S_with_per_group_thresholds(
        eta_S2, y_S2, a_t, m_t, thr_group_t, class_to_group, K
        # ‚ùå No class_weights!
    )
```

**After**:
```python
def worst_group_eg_outer(eta_S1, y_S1, eta_S2, y_S2, class_to_group, K,
                         T=30, xi=0.2, class_weights=None, **inner_kwargs):
    # ... optimization WITH reweighting
    w_err, gerrs = worst_error_on_S_with_per_group_thresholds(
        eta_S2, y_S2, a_t, m_t, thr_group_t, class_to_group, K,
        class_weights=class_weights  # ‚úÖ Pass weights!
    )
```

#### 2. Propagated Weights Through Inner Loop

```python
def inner_cost_sensitive_plugin_with_per_group_thresholds(
    eta_S1, y_S1, eta_S2, y_S2, class_to_group, K,
    beta, lambda_grid, class_weights=None, **kwargs):
    # ... all error computations now use class_weights
    w_err, gerrs = worst_error_on_S_with_per_group_thresholds(
        eta_S2, y_S2, a_cur, mu, t_group_cur, class_to_group, K,
        class_weights=class_weights  # ‚úÖ Consistent reweighting
    )
```

#### 3. Connected to Main Pipeline (`gse_balanced_plugin.py`)

```python
# Load class weights from splits directory
class_weights = load_class_weights(CONFIG['dataset']['splits_dir'])

# Pass to EG-outer
alpha_star, mu_star, t_group_star, beta_star, eg_hist = worst_group_eg_outer(
    eta_S1.to(DEVICE), y_S1.to(DEVICE),
    eta_S2.to(DEVICE), y_S2.to(DEVICE),
    class_to_group.to(DEVICE), K=num_groups,
    class_weights=class_weights,  # ‚úÖ CRITICAL addition!
    **other_params
)
```

---

## üìà Expected Improvements

### Before Fix (Broken)
```
EG iteration 1/30:
  Œ≤=[0.5000, 0.5000]  ‚Üê Uniform
  Œ±=[1.0000, 1.0000]  ‚Üê Never changes
  Œº=[-0.33, 0.33]     ‚Üê Weak
  Worst error: 0.22   ‚Üê Misleading (balanced test)

Early stop at iter 7 (no learning signal)

Deployment: Worst error = 0.70 üò± (actual long-tail)
```

### After Fix (Expected)
```
EG iteration 1/40:
  Œ≤=[0.5000, 0.5000]  ‚Üê Uniform start
  Œ±=[0.9500, 1.0500]  ‚Üê Learning!
  Œº=[-0.65, 0.65]     ‚Üê Stronger
  Worst error: 0.35 (reweighted)

EG iteration 20/40:
  Œ≤=[0.3845, 0.6155]  ‚Üê Adapted to tail
  Œ±=[0.8103, 1.2340]  ‚Üê Converged (23% tail boost)
  Œº=[-0.97, 0.97]     ‚Üê Strong signal
  Worst error: 0.29 (reweighted)

Deployment: Worst error = 0.30 ‚úÖ (matches prediction)
```

---

## üéØ Verification Checklist

When running fixed version, verify:

### 1. ‚úÖ Weights Loaded
```bash
‚úÖ Loaded class weights from data/cifar100_lt_if100_splits_fixed/class_weights.json
   min=0.000144, max=0.0144 (100x ratio)
```

### 2. ‚úÖ Reweighting Enabled
```bash
Reweighting: ‚úÖ ENABLED
# Should NOT see: ‚ùå DISABLED (may fail on balanced data!)
```

### 3. ‚úÖ Alpha Evolution
```bash
[Inner 1] Œ±=[1.0000, 1.0000] (Œîmax=0.0000)  ‚Üê Initial
[Inner 3] Œ±=[0.9650, 1.0350] (Œîmax=0.0350)  ‚Üê Learning!
[Inner 8] Œ±=[0.9234, 1.0866] (Œîmax=0.0142)  ‚Üê Converging
```

### 4. ‚úÖ Reweighted Errors Show Gap
```bash
# Head vs Tail error gap should be MUCH larger with reweighting:
Head error (reweighted) = 0.28
Tail error (reweighted) = 0.65  ‚Üê 2-3x higher!
```

### 5. ‚úÖ Convergence Not Premature
```bash
# Should NOT see early stop before iter 15+:
‚èπ Early stop EG at iter 22, best worst=0.2893  ‚Üê Good!
```

### 6. ‚úÖ Stronger Mu Values
```bash
Œº* = [-0.9749, 0.9749]  ‚Üê Range ~1.95 (3x stronger than broken version)
```

---

## üîç Additional Fixes Applied

Beyond reweighting, also fixed:

### 1. **Ground-Truth Groups for Threshold Fitting**
```python
# Before (WRONG):
t_group = fit_group_thresholds(raw_margins, pred_groups, ...)  # ‚ùå Predicted groups

# After (CORRECT):
y_groups = class_to_group[y_S1]  # Ground-truth labels
t_group = fit_group_thresholds(raw_margins, y_groups, ...)  # ‚úÖ True groups
```

### 2. **Implemented Alpha Update Logic**
```python
# Before: placeholder "pass" statements
# After: Full conditional acceptance + EMA + projection
```

### 3. **Increased Patience and Reduced Beta Floor**
```python
patience: 6 ‚Üí 10       # Allow more iterations before early stop
beta_floor: 0.05 ‚Üí 0.02  # Allow stronger adaptation
```

---

## üìö Related Issues

- **Threshold inconsistency**: Fixed by using ground-truth groups
- **Early stopping**: Fixed by higher patience + stronger learning signal
- **Weak mu values**: Fixed by reweighting providing correct gradients

All stem from same root cause: **Missing reweighting in balanced validation**!

---

## Summary

**The single most critical fix**: Adding `class_weights` parameter throughout EG-outer optimization chain.

Without reweighting:
- ‚ùå Balanced validation ‚â† Long-tail deployment
- ‚ùå Alpha has no learning signal
- ‚ùå Test metrics are misleading
- ‚ùå Production performance fails

With reweighting:
- ‚úÖ Balanced validation reflects long-tail performance
- ‚úÖ Alpha learns group-specific acceptance
- ‚úÖ Test metrics predict deployment
- ‚úÖ Strong worst-group guarantees transfer to production

**Status**: Fix implemented, ready for testing with `python run_improved_eg_outer.py`
