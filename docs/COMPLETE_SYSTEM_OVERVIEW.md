# ğŸ“š AR-GSE Complete System Documentation

## ğŸ¯ Tá»•ng quan há»‡ thá»‘ng

**AR-GSE (Adaptive Rejection Gated Selective Ensemble)** lÃ  má»™t há»‡ thá»‘ng phá»©c táº¡p vÃ  hoÃ n chá»‰nh cho **selective prediction** trÃªn dá»¯ liá»‡u **long-tail imbalanced**. Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho CIFAR-100-LT vá»›i imbalance factor 100.

---

## ğŸ—ï¸ Kiáº¿n trÃºc tá»•ng thá»ƒ

```
Input: CIFAR-100 Raw Dataset
           â†“
    [1. Data Pipeline]
    â”œâ”€â”€ CIFAR-100-LT Creation (exponential decay)
    â”œâ”€â”€ Head/Tail Grouping (threshold=20)
    â”œâ”€â”€ Strategic Splits (train/tuneV/val_lt/test_lt)  
    â””â”€â”€ Proportional Duplication
           â†“
    [2. Expert Training]
    â”œâ”€â”€ CE Baseline Expert
    â”œâ”€â”€ LogitAdjust Expert  
    â”œâ”€â”€ BalancedSoftmax Expert
    â””â”€â”€ Temperature Calibration
           â†“
    [3. Gating Training]  
    â”œâ”€â”€ Rich Feature Engineering (24D)
    â”œâ”€â”€ Two-Stage Training (pretrain + selective)
    â””â”€â”€ Pinball Loss Optimization
           â†“
    [4. Ensemble Training]
    â”œâ”€â”€ GSE-Balanced Plugin (unconstrained)
    â”œâ”€â”€ GSE-Constrained Plugin (with fairness)
    â””â”€â”€ Primal-Dual Optimization
           â†“
    Output: Optimized AR-GSE Model
```

---

## ğŸ“ Cáº¥u trÃºc Documentation

### 1. Dataset System (`docs/dataset.md`)
**Má»¥c Ä‘Ã­ch**: Táº¡o vÃ  quáº£n lÃ½ CIFAR-100-LT dataset vá»›i long-tail distribution

**Key Components**:
- `enhanced_datasets.py`: CIFAR100LTDataset vá»›i exponential sampling
- `dataloader_utils.py`: DataLoader creation vÃ  caching
- `groups.py`: Head/tail grouping logic
- `splits.py`: Strategic splitting cho training phases

**Input/Output**:
- Input: Raw CIFAR-100 dataset  
- Output: Structured long-tail splits vá»›i proper imbalance

**Technical Highlights**:
- Exponential decay: `n_k = n_max Ã— (imb_factor)^(-k/N-1)`
- Proportional duplication cho val/test splits
- Group-aware splitting strategy

### 2. Expert Training (`docs/experts_training.md`)
**Má»¥c Ä‘Ã­ch**: Train 3 chuyÃªn gia specialists vÃ  calibrate outputs

**Key Components**:
- `train_experts.py`: Multi-expert training pipeline
- `experts.py`: Expert model definitions  
- `losses.py`: Specialized loss functions
- `backbones/resnet_cifar.py`: ResNet architectures

**Input/Output**:
- Input: CIFAR-100-LT splits
- Output: Calibrated expert logits cho ensemble

**Technical Highlights**:
- 3 experts: CE, LogitAdjust, BalancedSoftmax
- Temperature scaling calibration
- Cross-validation cho robust performance

### 3. Gating Training (`docs/gating_training.md`)  
**Má»¥c Ä‘Ã­ch**: Train gating network Ä‘á»ƒ weight experts dynamically

**Key Components**:
- `train_gating_only.py`: Two-stage gating training
- `gating.py`: Gating network architecture
- Rich feature engineering (24 dimensions)

**Input/Output**:
- Input: Expert logits + metadata features
- Output: Pre-trained gating weights

**Technical Highlights**:
- Two-stage: pretrain (CE) + selective (Pinball)
- Rich features: confidence, entropy, disagreement, etc.
- Alternating optimization strategy

### 4. Ensemble Training (`docs/ensemble_training.md`)
**Má»¥c Ä‘Ã­ch**: Optimize final ensemble parameters via plugin algorithms

**Key Components**:
- `gse_balanced_plugin.py`: Unconstrained optimization
- `gse_constrained_plugin.py`: Constrained with fairness
- Primal-dual optimization methods

**Input/Output**:
- Input: Expert logits + pre-trained gating
- Output: Optimal (Î±*, Î¼*, t*) parameters

**Technical Highlights**:
- Fixed-point Î± updates
- Grid search Î¼ optimization  
- Lagrangian dual methods cho constraints

---

## ğŸ›ï¸ Key Parameters vÃ  Variables

### Global Configuration
```python
DATASET_CONFIG = {
    'name': 'cifar100_lt_if100',
    'num_classes': 100,
    'imb_factor': 100,
    'grouping_threshold': 20,  # Head vs tail split
}

TRAINING_CONFIG = {
    'experts': ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline'],
    'batch_size': 128,
    'epochs': 200,
    'learning_rate': 0.1,
    'weight_decay': 5e-4,
}
```

### Ensemble Parameters  
```python
# Core optimization variables
Î± = [Î±_head, Î±_tail]     # Per-group scaling [K,]
Î¼ = [Î¼_head, Î¼_tail]     # Per-group bias [K,]  
t = threshold            # Decision threshold (scalar)

# Mixture posteriors
Î·Ìƒ(x) = Î£_e w^(e)(x) * p^(e)(y|x)   # [N, C]

# Decision rule  
accept = max_y Î±_{g(y)} * Î·Ìƒ_y - Î£_y (1/Î±_{g(y)} - Î¼_{g(y)}) * Î·Ìƒ_y â‰¥ t
```

---

## ğŸ“Š Performance Metrics

### Primary Objectives
- **Balanced Error**: `(1/K) Î£_k e_k` - Fair across groups
- **Worst-case Error**: `max_k e_k` - Robust performance  
- **Coverage**: Acceptance rate while maintaining quality
- **Per-group Fairness**: Equal error rates across head/tail

### Evaluation Protocol
```python
# Metrics on validation set
e_k = per_group_error_rates     # [K,] 
cov_k = per_group_coverage      # [K,]
accuracy = correct / accepted    # Overall accuracy
aurc = area_under_rc_curve      # Calibration quality
```

---

## ğŸš€ Usage Workflow

### Complete Training Pipeline
```bash
# 1. Prepare dataset (if needed)
python create_splits_advanced.py

# 2. Train expert models  
python train_experts.py

# 3. Train gating network
python train_gating.py

# 4a. Run balanced ensemble optimization
python -m src.train.gse_balanced_plugin

# 4b. OR run constrained optimization  
python -m src.train.gse_constrained_plugin

# 5. Evaluate final model
python comprehensive_inference.py
```

### Quick Demo
```bash
# Run synthetic demo Ä‘á»ƒ hiá»ƒu algorithms
python demo_gse_plugins.py
```

---

## ğŸ”¬ Algorithm Insights

### 1. **Progressive Complexity**
- Dataset â†’ Experts â†’ Gating â†’ Ensemble
- Má»—i stage builds on previous outputs
- Modular design cho flexibility

### 2. **Multi-Expert Strategy**
- CE: General classification baseline
- LogitAdjust: Long-tail specific adjustments  
- BalancedSoftmax: Balanced training approach
- Ensemble combines strengths

### 3. **Selective Prediction Framework**
- Reject uncertain predictions
- Maintain high accuracy on accepted samples
- Balance coverage vs quality tradeoff

### 4. **Fairness-Aware Optimization**
- GSE-Balanced: Direct performance optimization
- GSE-Constrained: Explicit fairness constraints
- Primal-dual methods cho principled solutions

---

## ğŸ“ˆ Expected Results

### Typical Performance (CIFAR-100-LT IF=100)
```python
# After full training pipeline:
balanced_error â‰ˆ 0.08-0.12      # 8-12% balanced error
worst_error â‰ˆ 0.15-0.20         # 15-20% worst group error  
coverage â‰ˆ 0.65-0.75             # 65-75% acceptance rate
head_accuracy â‰ˆ 0.92-0.95        # 92-95% trÃªn head classes
tail_accuracy â‰ˆ 0.85-0.90        # 85-90% trÃªn tail classes
```

### Key Improvements over Baselines
- **+5-10%** accuracy improvement on tail classes
- **More balanced** performance across head/tail groups  
- **Higher coverage** at same error rates
- **Better calibration** cho uncertainty estimation

---

## ğŸ› ï¸ Customization Points

### 1. **Dataset Adaptation**
- Change `imb_factor` cho different imbalance levels
- Adjust `grouping_threshold` cho different group sizes
- Modify splitting strategy trong `create_splits_advanced.py`

### 2. **Expert Selection**  
- Add/remove expert types trong `train_experts.py`
- Experiment vá»›i different loss functions
- Try different backbone architectures

### 3. **Gating Features**
- Modify feature engineering trong `gating.py`
- Experiment vá»›i different gating architectures
- Try alternative training objectives

### 4. **Ensemble Optimization**
- Tune hyperparameters trong plugin algorithms
- Try different constraint formulations
- Experiment vá»›i multi-objective optimization

---

## ğŸ¯ Summary

**AR-GSE** lÃ  má»™t há»‡ thá»‘ng hoÃ n chá»‰nh vÃ  sophisticated cho selective prediction trÃªn long-tail data. Vá»›i 4 giai Ä‘oáº¡n training Ä‘Æ°á»£c thiáº¿t káº¿ cáº©n tháº­n, há»‡ thá»‘ng Ä‘áº¡t Ä‘Æ°á»£c:

âœ… **High Performance**: Superior accuracy trÃªn both head vÃ  tail classes  
âœ… **Fairness**: Balanced performance across imbalanced groups
âœ… **Flexibility**: Modular design cho easy customization  
âœ… **Robustness**: Multiple experts vÃ  principled ensemble methods
âœ… **Practical**: Ready-to-use pipeline vá»›i comprehensive documentation

ToÃ n bá»™ codebase Ä‘Ã£ Ä‘Æ°á»£c document chi tiáº¿t vá»›i examples, configurations, vÃ  usage instructions Ä‘á»ƒ báº¡n cÃ³ thá»ƒ "thi thoáº£ng Ä‘á»c láº¡i" vÃ  understand deeply tá»«ng component cá»§a há»‡ thá»‘ng.

**CÃ¡c file documentation quan trá»ng**:
- `docs/dataset.md` - Data pipeline system
- `docs/experts_training.md` - Expert models training  
- `docs/gating_training.md` - Gating network training
- `docs/ensemble_training.md` - Final ensemble optimization
- `demo_gse_plugins.py` - Interactive demonstration

Há»‡ thá»‘ng AR-GSE thá»ƒ hiá»‡n state-of-the-art approach cho selective prediction vá»›i long-tail data, combining multiple advanced techniques trong má»™t pipeline cohesive vÃ  well-engineered.