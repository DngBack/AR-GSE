# ğŸ“˜ HÆ°á»›ng Dáº«n Recompute Logits

## ğŸ¯ Má»¥c ÄÃ­ch

Khi báº¡n cÃ³ **expert weights Ä‘Ã£ train** nhÆ°ng **data splits má»›i**, thay vÃ¬ train láº¡i experts (máº¥t ~4 giá»), chá»‰ cáº§n **recompute logits** (máº¥t ~15 phÃºt).

---

## âš™ï¸ Chuáº©n Bá»‹

### 1. Kiá»ƒm Tra Expert Checkpoints

```bash
ls checkpoints/experts/cifar100_lt_if100/
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
best_ce_baseline.pth
best_balsoftmax_baseline.pth
best_logitadjust_baseline.pth
best_decoupling_twostage.pth (optional)
```

### 2. Kiá»ƒm Tra Data Splits Má»›i

```bash
ls data/cifar100_lt_if100_splits_fixed/
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
train_indices.json
test_indices.json (8,000 samples)
val_indices.json (1,000 samples)
tunev_indices.json (1,000 samples)
class_weights.json
train_class_counts.json
```

---

##  ğŸš€ CÃ¡ch Cháº¡y

### Option 1: Táº¥t Cáº£ Experts (Recommended)

```bash
python3 recompute_logits.py \
    --experts ce_baseline balsoftmax_baseline logitadjust_baseline \
    --checkpoint-prefix best_
```

### Option 2: Custom Settings

```bash
python3 recompute_logits.py \
    --experts ce_baseline balsoftmax_baseline \
    --checkpoint-prefix best_ \
    --splits val test \
    --batch-size 64 \
    --output-dir outputs/logits_new
```

### Option 3: Final Calibrated Checkpoints

```bash
python3 recompute_logits.py \
    --experts ce_baseline balsoftmax_baseline logitadjust_baseline \
    --checkpoint-prefix final_calibrated_
```

---

## ğŸ“Š Output Structure

```
outputs/logits_fixed/
â”œâ”€â”€ ce_baseline/
â”‚   â”œâ”€â”€ val_logits.npz       # 1,000 samples
â”‚   â”œâ”€â”€ test_logits.npz      # 8,000 samples
â”‚   â””â”€â”€ tunev_logits.npz     # 1,000 samples
â”œâ”€â”€ balsoftmax_baseline/
â”‚   â”œâ”€â”€ val_logits.npz
â”‚   â”œâ”€â”€ test_logits.npz
â”‚   â””â”€â”€ tunev_logits.npz
â””â”€â”€ logitadjust_baseline/
    â”œâ”€â”€ val_logits.npz
    â”œâ”€â”€ test_logits.npz
    â””â”€â”€ tunev_logits.npz
```

### Format cá»§a `.npz` file

```python
import numpy as np

data = np.load('outputs/logits_fixed/ce_baseline/val_logits.npz')

# Available arrays:
data['logits']       # Shape: (N, 100) - raw logits
data['targets']      # Shape: (N,) - true labels  
data['predictions']  # Shape: (N,) - predicted labels (argmax of logits)
```

---

## âœ… Verify Logits

Sau khi recompute, verify Ä‘á»ƒ Ä‘áº£m báº£o Ä‘Ãºng:

```bash
python3 verify_logits.py
```

**Check list:**
- âœ“ Shape consistency
- âœ“ Predictions match argmax
- âœ“ Accuracy reasonable (>60%)
- âœ“ No NaN or Inf values
- âœ“ Reweighted metrics computed correctly

---

## ğŸ”§ Troubleshooting

### Lá»—i 1: Checkpoint not found

**NguyÃªn nhÃ¢n:** TÃªn expert khÃ´ng Ä‘Ãºng hoáº·c checkpoint khÃ´ng tá»“n táº¡i

**Giáº£i phÃ¡p:**
```bash
# List available checkpoints
ls checkpoints/experts/cifar100_lt_if100/

# Use exact names
python3 recompute_logits.py --experts ce_baseline balsoftmax_baseline
```

### Lá»—i 2: Model architecture mismatch

**Error:**
```
RuntimeError: Error(s) in loading state_dict for Expert:
    Missing key(s): ...
    Unexpected key(s): ...
```

**NguyÃªn nhÃ¢n:** Model class khÃ´ng khá»›p vá»›i checkpoint

**Giáº£i phÃ¡p:** Script Ä‘Ã£ Ä‘Æ°á»£c update Ä‘á»ƒ tá»± Ä‘á»™ng load `Expert` class tá»« `src.models.experts`

### Lá»—i 3: Size mismatch

**Error:**
```
size mismatch for fc.weight: copying a param with shape torch.Size([100, 64]) 
from checkpoint, the shape in current model is torch.Size([100, 512]).
```

**NguyÃªn nhÃ¢n:** Backbone khÃ¡c nhau (ResNet-32 vs ResNet-18)

**Giáº£i phÃ¡p:** Äáº£m báº£o dÃ¹ng Ä‘Ãºng backbone. Script sá»­ dá»¥ng `cifar_resnet32` theo máº·c Ä‘á»‹nh.

### Lá»—i 4: CUDA out of memory

**Giáº£i phÃ¡p:**
```bash
python3 recompute_logits.py --batch-size 64
# hoáº·c
python3 recompute_logits.py --batch-size 32
```

### Lá»—i 5: Wrong logits shape

**NguyÃªn nhÃ¢n:** Data splits má»›i khÃ¡c size vá»›i cÅ©

**Kiá»ƒm tra:**
```bash
# Check splits size
python3 -c "
import json
with open('data/cifar100_lt_if100_splits_fixed/val_indices.json') as f:
    print(f'Val: {len(json.load(f))} samples')
with open('data/cifar100_lt_if100_splits_fixed/test_indices.json') as f:
    print(f'Test: {len(json.load(f))} samples')
"
```

---

## ğŸ” Advanced: Inspect Checkpoint

Náº¿u gáº·p lá»—i, inspect checkpoint Ä‘á»ƒ hiá»ƒu cáº¥u trÃºc:

```python
import torch

checkpoint = torch.load(
    'checkpoints/experts/cifar100_lt_if100/best_ce_baseline.pth',
    map_location='cpu'
)

print("Keys:", checkpoint.keys())

# Get state dict
state_dict = checkpoint.get('model_state_dict', checkpoint.get('state_dict', checkpoint))

# Print first few parameters
for i, (k, v) in enumerate(list(state_dict.items())[:10]):
    print(f"{k}: {v.shape}")
```

---

## ğŸ“ Workflow HoÃ n Chá»‰nh

```bash
# 1. Táº¡o data splits má»›i
python3 create_splits_fixed.py

# 2. Recompute logits
python3 recompute_logits.py \
    --experts ce_baseline balsoftmax_baseline logitadjust_baseline \
    --checkpoint-prefix best_

# 3. Verify
python3 verify_logits.py

# 4. Train gating (náº¿u logits OK)
python3 train_gating.py --mode pretrain

# 5. Continue training
python3 train_gating.py --mode selective
python3 train_argse.py
```

---

## ğŸ’¡ Tips

1. **Use best vs final_calibrated:**
   - `best_*`: Best validation accuracy during training
   - `final_calibrated_*`: After temperature scaling
   - ThÆ°á»ng dÃ¹ng `best_` cho logits

2. **Batch size:**
   - Default 128 works for most GPUs
   - Giáº£m xuá»‘ng 64/32 náº¿u out of memory
   - TÄƒng lÃªn 256 náº¿u GPU lá»›n â†’ faster

3. **Parallel processing:**
   Script cháº¡y tuáº§n tá»± tá»«ng expert. Náº¿u muá»‘n nhanh hÆ¡n, cháº¡y song song:
   ```bash
   python3 recompute_logits.py --experts ce_baseline &
   python3 recompute_logits.py --experts balsoftmax_baseline &
   python3 recompute_logits.py --experts logitadjust_baseline &
   wait
   ```

4. **Reuse logits:**
   Logits chá»‰ cáº§n compute 1 láº§n. Sau Ä‘Ã³ cÃ³ thá»ƒ dÃ¹ng láº¡i cho nhiá»u experiments.

---

## â±ï¸ Performance

| Expert | Samples | Time (GPU) | Memory |
|--------|---------|------------|---------|
| CE | 10,000 | ~2 min | ~2 GB |
| BalSoftmax | 10,000 | ~2 min | ~2 GB |
| LogitAdjust | 10,000 | ~2 min | ~2 GB |
| **Total** | **30,000** | **~6 min** | **~2 GB** |

Vs train tá»« Ä‘áº§u: **~4 hours** â†’ Tiáº¿t kiá»‡m **97.5%** thá»i gian! ğŸš€

---

## âœ… Success Criteria

Sau khi recompute, check:

1. **Files exist:**
   ```bash
   ls outputs/logits_fixed/*/val_logits.npz
   ls outputs/logits_fixed/*/test_logits.npz
   ls outputs/logits_fixed/*/tunev_logits.npz
   ```

2. **Accuracy reasonable:**
   ```
   CE: ~70-75%
   BalSoftmax: ~72-77%
   LogitAdjust: ~71-76%
   ```

3. **Shape correct:**
   ```
   Val: (1000, 100)
   Test: (8000, 100)
   TuneV: (1000, 100)
   ```

4. **No errors in verify:**
   ```bash
   python3 verify_logits.py  # Should show âœ“ for all checks
   ```

---

BÃ¢y giá» báº¡n cÃ³ thá»ƒ proceed vá»›i gating training! ğŸ¯
