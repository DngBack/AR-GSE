# Gating Network cho MAP (Mixture-Aware Plug-in) vá»›i L2R

## ğŸ“š Tá»•ng quan

Implementation Ä‘áº§y Ä‘á»§ cá»§a **Gating Network** theo Ã½ tÆ°á»Ÿng MAP + L2R cho long-tailed classification vá»›i selective rejection.

### Kiáº¿n trÃºc tá»•ng thá»ƒ

```
Expert Logits â†’ Posteriors â†’ Gating Network â†’ Mixture Posterior â†’ MAP Selector â†’ Accept/Reject
   [N,E,C]        [N,E,C]      (features)        [N,C]         (margin-Î³U)
```

## ğŸ¯ Má»¥c tiÃªu

1. **Há»c trá»ng sá»‘ mixture** Ä‘á»ƒ tá»‘i Ä‘a hÃ³a likelihood cá»§a phÃ¢n phá»‘i há»—n há»£p
2. **Táº­n dá»¥ng diversity** cá»§a experts qua uncertainty/disagreement features
3. **CÃ¢n báº±ng load** giá»¯a cÃ¡c experts (trÃ¡nh collapse khi dÃ¹ng Top-K)
4. **Cung cáº¥p uncertainty signal** cho MAP margin: `m_MAP(x) = m_L2R(x) - Î³Â·U(x)`

## ğŸ“ Files Ä‘Ã£ triá»ƒn khai

### 1. `src/models/gating_network_map.py`
**Gating Network Core - Architecture & Feature Extraction**

#### Components:

##### a) `UncertaintyDisagreementFeatures`
TÃ­nh cÃ¡c features uncertainty/disagreement tá»« expert posteriors:

**Per-expert features:**
- `expert_entropy`: H(p^(e)) - Ä‘á»™ khÃ´ng cháº¯c cháº¯n cá»§a má»—i expert
- `expert_confidence`: max(p^(e)) - confidence cao nháº¥t
- `expert_margin`: p_1 - p_2 (top-1 vs top-2 gap)

**Disagreement features:**
- `disagreement_ratio`: tá»‰ lá»‡ experts báº¥t Ä‘á»“ng vá» top-1 prediction
- `mean_pairwise_kl`: trung bÃ¬nh KL divergence giá»¯a cÃ¡c cáº·p experts

**Mixture features:**
- `mixture_entropy`: H(Î·Ìƒ) - entropy cá»§a phÃ¢n phá»‘i há»—n há»£p
- `posterior_variance`: variance cá»§a posteriors giá»¯a experts
- `mutual_information`: I(Y; E | X) â‰ˆ H(Î·Ìƒ) - mean(H(p^(e)))

**LÃ½ do:**
> Deep Ensembles cho tháº¥y cÃ¡c Ä‘áº¡i lÆ°á»£ng nÃ y tÆ°Æ¡ng quan máº¡nh vá»›i sai sá»‘ & overconfidence. DÃ¹ng lÃ m feature giÃºp router "biáº¿t khi nÃ o nÃªn dÃ¨ chá»«ng".

##### b) `GatingFeatureExtractor`
Concat táº¥t cáº£ features thÃ nh vector Ä‘áº§u vÃ o:
```
Input:  posteriors [B, E, C]
Output: features   [B, D] vá»›i D = E*C + 3*E + 5
```

##### c) `GatingMLP`
MLP nÃ´ng (2-3 layers) vá»›i **LayerNorm** (stable hÆ¡n BatchNorm cho batch nhá»):
```python
Input â†’ Linear â†’ LayerNorm â†’ ReLU â†’ Dropout â†’ ... â†’ Output
[B,D]                                                  [B,E]
```

##### d) Routing Strategies

**Dense Softmax (Jordan & Jacobs 1994):**
```python
w_Ï†(x) = softmax(g(x))  # All experts used
```

**Noisy Top-K (Shazeer et al. 2017):**
```python
1. Add Gaussian noise: g' = g + N(0, ÏƒÂ²)
2. Select Top-K experts
3. Softmax & renormalize over K
```

##### e) `GatingNetwork` (Complete)
Káº¿t há»£p táº¥t cáº£:
```python
posteriors â†’ feature_extractor â†’ mlp â†’ router â†’ weights
  [B,E,C]         [B,D]           [B,E]  [B,E]
```

**Methods:**
- `forward(posteriors)`: returns `(weights, aux_outputs)`
- `get_mixture_posterior(posteriors, weights)`: tÃ­nh Î·Ìƒ(x) = Î£ w_e Â· p^(e)

##### f) `compute_uncertainty_for_map`
TÃ­nh U(x) cho MAP margin:
```
U(x) = aÂ·H(w_Ï†) + bÂ·Disagree({p^(e)}) + dÂ·H(Î·Ìƒ)
```

---

### 2. `src/models/gating_losses.py`
**Loss Functions - Maximum Likelihood + Regularization**

#### Loss Components:

##### a) `MixtureNLLLoss` (CORE - Báº¯t buá»™c)
Maximum likelihood cho mixture model:
```python
L_mix = -E_{(x,y)} log(Î£_e w_Ï†(x)_e Â· p^(e)(y|x))
```

**LÃ½ thuyáº¿t:**
- Proper scoring rule â†’ khuyáº¿n khÃ­ch calibration
- Nháº¥t quÃ¡n vá»›i EM algorithm cho HME (Jordan & Jacobs 1994)
- Kháº£ vi â†’ backprop end-to-end

##### b) `LoadBalancingLoss` (Cho Top-K routing)
Switch Transformer (Fedus et al. 2021):
```python
L_LB = Î± Â· N Â· Î£_i f_i Â· P_i

f_i = fraction of tokens routed to expert i (hard assignment)
P_i = average router probability for expert i
Î± â‰ˆ 1e-2 (balancing coefficient)
```

**Intuition:**
- Expert i nháº­n nhiá»u tokens (f_i cao) VÃ€ cÃ³ xÃ¡c suáº¥t cao (P_i cao) â†’ penalty
- Khuyáº¿n khÃ­ch phÃ¢n bá»• Ä‘á»u â†’ trÃ¡nh collapse

##### c) `EntropyRegularizer` (TÃ¹y chá»n)
```python
L_H = -H(w) = Î£_e w_e log(w_e)  (maximize entropy)
hoáº·c
L_H = H(w)                       (minimize entropy)
```

Trong MAP: maximize entropy â†’ khuyáº¿n khÃ­ch diversity

##### d) `GatingLoss` (Combined)
```python
L_total = L_mix + Î»_LBÂ·L_LB + Î»_HÂ·L_H

Defaults:
- Î»_LB = 1e-2  (Switch Transformer recommendation)
- Î»_H = 0.01   (light regularization)
```

#### Utility Functions:

##### `compute_gating_metrics`
Monitor metrics:
- `gating_entropy`: H(wÌ„) - diversity cá»§a routing
- `load_std/max/min`: cÃ¢n báº±ng expert usage
- `mixture_acc`: accuracy cá»§a mixture
- `effective_experts`: exp(H(mean_weights)) - sá»‘ experts thá»±c sá»± Ä‘Æ°á»£c dÃ¹ng

---

### 3. `src/train/train_gating_map.py`
**Training Script - End-to-End Pipeline**

#### Workflow:

```
1. Load expert logits (Ä‘Ã£ calibrated)
   â”œâ”€ gating split: training
   â””â”€ val split: validation (balanced)

2. Convert logits â†’ posteriors

3. Initialize GatingNetwork
   â”œâ”€ Feature extractor
   â”œâ”€ MLP
   â””â”€ Router (dense/top-k)

4. Training loop
   â”œâ”€ Forward: posteriors â†’ weights
   â”œâ”€ Loss: L_mix + L_LB + L_H
   â”œâ”€ Backward + optimizer step
   â””â”€ Validation every N epochs

5. Save best model (by balanced_acc)

6. Export checkpoints & training history
```

#### Configuration:

```python
CONFIG = {
    'gating': {
        # Architecture
        'hidden_dims': [256, 128],
        'dropout': 0.1,
        'activation': 'relu',
        
        # Routing
        'routing': 'dense',  # or 'top_k'
        'top_k': 2,
        
        # Training
        'epochs': 100,
        'batch_size': 128,
        'lr': 1e-3,
        'optimizer': 'adamw',
        'scheduler': 'cosine',
        'warmup_epochs': 5,
        
        # Loss
        'lambda_lb': 1e-2,
        'lambda_h': 0.01,
        
        # Long-tail
        'use_class_weights': True,
    }
}
```

#### Key Functions:

- `load_expert_logits()`: load tá»« disk
- `create_dataloaders()`: táº¡o train/val loaders
- `train_one_epoch()`: train loop vá»›i gradient clipping
- `validate()`: compute metrics trÃªn val set
- `compute_group_accuracies()`: head/tail accuracy

---

### 4. `src/train/test_gating_map.py`
**Test Suite - Verification**

Tests:
1. âœ… Uncertainty feature extraction
2. âœ… Gating network forward pass
3. âœ… Loss computation
4. âœ… Metrics computation
5. âœ… U(x) for MAP
6. âœ… Gradient flow
7. âœ… Real expert logits (if available)

Run: `python src/train/test_gating_map.py`

---

## ğŸš€ Usage

### Step 1: Train Experts (Already done)
```bash
python src/train/train_expert.py
```

Output: expert logits táº¡i `outputs/logits/cifar100_lt_if100/`

### Step 2: Test Gating Implementation
```bash
python src/train/test_gating_map.py
```

Expected: All tests pass âœ…

### Step 3: Train Gating Network

**Dense routing (all experts):**
```bash
python src/train/train_gating_map.py --routing dense
```

**Top-K routing (sparse):**
```bash
python src/train/train_gating_map.py --routing top_k --top_k 2 --lambda_lb 1e-2
```

**Custom hyperparameters:**
```bash
python src/train/train_gating_map.py \
    --routing dense \
    --epochs 150 \
    --batch_size 64 \
    --lr 5e-4 \
    --lambda_lb 1e-2 \
    --lambda_h 0.02
```

### Step 4: Check Results

Checkpoints:
- `checkpoints/gating_map/cifar100_lt_if100/best_gating.pth`
- `checkpoints/gating_map/cifar100_lt_if100/final_gating.pth`

Training history:
- `results/gating_map/cifar100_lt_if100/training_history.json`

---

## ğŸ“Š Expected Behavior

### Gating Entropy
- **High entropy** (close to log(E)): experts Ä‘Æ°á»£c dÃ¹ng Ä‘á»u â†’ diversity cao
- **Low entropy**: chá»‰ má»™t vÃ i experts dominant â†’ cÃ³ thá»ƒ collapse

### Load Balancing
- **Std of expert usage < 0.1**: cÃ¢n báº±ng tá»‘t
- **Max usage < 0.5**: khÃ´ng cÃ³ expert nÃ o quÃ¡ dominant

### Mixture Accuracy
- **> Individual expert accs**: ensemble effect
- **Balanced acc** (head & tail gáº§n nhau): fair performance

### Effective Experts
- Close to E: táº¥t cáº£ experts cÃ³ Ã­ch
- Close to 1: collapse (chá»‰ 1 expert Ä‘Æ°á»£c dÃ¹ng)

---

## ğŸ”¬ LÃ½ thuyáº¿t ná»n táº£ng

### 1. Mixture of Experts (MoE)
```
p(y|x) = Î£_e w_Ï†(x)_e Â· p^(e)(y|x)

w_Ï†(x): gating function (input-dependent weights)
p^(e)(y|x): expert e's prediction
```

**Huáº¥n luyá»‡n:** Maximum likelihood
```
max_Ï† E_{(x,y)} log p(y|x) = E log(Î£_e w_Ï†(x)_e Â· p^(e)(y|x))
```

**References:**
- Jordan & Jacobs (1994): Hierarchical Mixtures of Experts and EM
- Jacobs et al. (1991): Adaptive Mixtures of Local Experts

### 2. Load Balancing (Switch Transformer)
**Problem:** Top-K routing cÃ³ thá»ƒ collapse â†’ chá»‰ dÃ¹ng 1-2 experts

**Solution:** Auxiliary loss khuyáº¿n khÃ­ch cÃ¢n báº±ng
```
L_LB = Î± Â· N Â· Î£_i f_i Â· P_i
```

**References:**
- Fedus et al. (2021): Switch Transformers - Scaling to Trillion Parameters
- Shazeer et al. (2017): Outrageously Large Neural Networks (Sparsely-Gated MoE)

### 3. Uncertainty Quantification (Deep Ensembles)
**Key Insight:** Entropy/disagreement cá»§a ensemble tÆ°Æ¡ng quan vá»›i error

**Application:** DÃ¹ng lÃ m features cho gating + U(x) cho MAP margin

**References:**
- Lakshminarayanan et al. (2017): Simple and Scalable Predictive Uncertainty
- Gal & Ghahramani (2016): Dropout as Bayesian Approximation

### 4. MAP Margin vá»›i Uncertainty Penalty
```
m_MAP(x) = m_L2R(x) - Î³Â·U(x)

m_L2R: margin tá»« L2R (tuyáº¿n tÃ­nh trÃªn Î·Ìƒ)
U(x): uncertainty tá»« mixture (H(w), disagreement, H(Î·Ìƒ))
Î³: penalty coefficient
```

**Intuition:**
- Khi experts tranh cÃ£i (high U) â†’ margin giáº£m â†’ dá»… reject hÆ¡n
- Táº­n dá»¥ng "wisdom of crowd": náº¿u ensemble khÃ´ng cháº¯c â†’ nÃªn tháº­n trá»ng

---

## ğŸ› ï¸ Customization

### ThÃªm features má»›i
Edit `UncertaintyDisagreementFeatures.compute()`:
```python
# VÃ­ dá»¥: thÃªm max KL divergence
max_kl = torch.max(kl_to_mean, dim=-1)[0]  # [B]
features['max_expert_kl'] = max_kl
```

### Thay Ä‘á»•i routing strategy
Táº¡o class má»›i káº¿ thá»«a `nn.Module`:
```python
class CustomRouter(nn.Module):
    def forward(self, logits):
        # Custom logic
        return weights
```

Update `GatingNetwork.__init__`:
```python
elif routing == 'custom':
    self.router = CustomRouter(...)
```

### ThÃªm loss term
Edit `GatingLoss.forward()`:
```python
# VÃ­ dá»¥: diversity loss khÃ¡c
loss_diversity = some_diversity_metric(weights)
total_loss += lambda_div * loss_diversity
```

---

## ğŸ› Troubleshooting

### Issue: Gating collapse (chá»‰ 1 expert Ä‘Æ°á»£c dÃ¹ng)
**Solutions:**
1. TÄƒng `lambda_h` (entropy regularization): 0.01 â†’ 0.05
2. TÄƒng `lambda_lb` (load-balancing): 1e-2 â†’ 5e-2
3. Giáº£m learning rate: 1e-3 â†’ 5e-4
4. ThÃªm warmup: `warmup_epochs = 10`

### Issue: Training khÃ´ng á»•n Ä‘á»‹nh (loss spike)
**Solutions:**
1. Enable gradient clipping: `grad_clip = 1.0`
2. DÃ¹ng LayerNorm thay BatchNorm (Ä‘Ã£ default)
3. Giáº£m learning rate
4. TÄƒng batch size

### Issue: Mixture acc tháº¥p hÆ¡n best expert
**NguyÃªn nhÃ¢n:** Gating chÆ°a há»c tá»‘t, routing khÃ´ng optimal

**Solutions:**
1. Train lÃ¢u hÆ¡n: epochs 100 â†’ 200
2. Simplify architecture: hidden_dims=[128] thay vÃ¬ [256,128]
3. Check features: in ra uncertainty metrics Ä‘á»ƒ xem cÃ³ informative khÃ´ng

### Issue: Validation acc tá»‘t nhÆ°ng test acc kÃ©m
**NguyÃªn nhÃ¢n:** Overfitting

**Solutions:**
1. TÄƒng dropout: 0.1 â†’ 0.3
2. TÄƒng weight_decay: 1e-4 â†’ 5e-4
3. Early stopping: save best model on val, stop khi khÃ´ng cáº£i thiá»‡n

---

## ğŸ“ˆ Next Steps

Sau khi train gating, báº¡n cÃ³ thá»ƒ:

1. **Integrate vá»›i MAP Selector:**
   ```python
   # Load gating
   gating = GatingNetwork(...)
   gating.load_state_dict(torch.load('best_gating.pth'))
   
   # Compute mixture & uncertainty
   weights, _ = gating(posteriors)
   mixture = gating.get_mixture_posterior(posteriors, weights)
   U = compute_uncertainty_for_map(posteriors, weights, mixture)
   
   # MAP margin
   m_MAP = m_L2R(mixture, alpha, mu, c) - gamma * U
   
   # Decision
   accept = (m_MAP >= 0)
   ```

2. **Optimize Î±, Î¼, Î³ trÃªn S1/S2:** (theo pipeline MAP)

3. **EG-outer cho worst-group:** (náº¿u cáº§n R_max)

4. **Conformal Risk Control:** (finite-sample guarantee)

---

## ğŸ“š References

### Mixture of Experts
1. Jordan & Jacobs (1994): Hierarchical Mixtures of Experts and the EM Algorithm
2. Jacobs et al. (1991): Adaptive Mixtures of Local Experts

### Sparse MoE & Load Balancing
3. Shazeer et al. (2017): Outrageously Large Neural Networks (Sparsely-Gated MoE)
4. Fedus et al. (2021): Switch Transformers
5. Lepikhin et al. (2020): GShard - Scaling Giant Models with Conditional Computation

### Uncertainty & Calibration
6. Lakshminarayanan et al. (2017): Simple and Scalable Predictive Uncertainty
7. Guo et al. (2017): On Calibration of Modern Neural Networks
8. Kull et al. (2019): Beyond Temperature Scaling (Dirichlet calibration)

### Selective Classification
9. Geifman & El-Yaniv (2017): Selective Classification for Deep Neural Networks
10. Geifman & El-Yaniv (2019): SelectiveNet - Learning to Abstain
11. Stutz et al. (2023): Learning to Reject with L2R

### Conformal Prediction
12. Angelopoulos et al. (2024): Conformal Risk Control

---

## âœ… Checklist Implementation

- [x] Feature extraction (uncertainty/disagreement)
- [x] Gating MLP vá»›i LayerNorm
- [x] Dense routing (softmax)
- [x] Noisy Top-K routing
- [x] Mixture NLL loss
- [x] Load-balancing loss (Switch)
- [x] Entropy regularization
- [x] Combined loss function
- [x] Training script vá»›i warmup/scheduler
- [x] Validation & metrics
- [x] Group-wise accuracy (head/tail)
- [x] Checkpoint saving
- [x] U(x) computation cho MAP
- [x] Test suite
- [x] Documentation

---

## ğŸ“§ Contact & Support

Náº¿u cÃ³ váº¥n Ä‘á», hÃ£y check:
1. Test suite: `python src/train/test_gating_map.py`
2. Loss values: NLL ~2-3, LB ~0.01, Entropy ~1-2
3. Metrics: mixture_acc > max(expert_accs), effective_experts â‰ˆ E

Happy training! ğŸš€
