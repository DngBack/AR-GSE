# üéØ AR-GSE Expert Training Documentation

T√†i li·ªáu chi ti·∫øt v·ªÅ h·ªá th·ªëng training expert models trong AR-GSE, bao g·ªìm ki·∫øn tr√∫c, loss functions, training pipeline, v√† calibration.

## üéØ T·ªïng quan

Expert training l√† **b∆∞·ªõc ƒë·∫ßu ti√™n** trong pipeline AR-GSE, t·∫°o ra 3 expert models v·ªõi c√°c chi·∫øn l∆∞·ª£c kh√°c nhau ƒë·ªÉ x·ª≠ l√Ω imbalanced data. M·ªói expert h·ªçc m·ªôt approach ri√™ng bi·ªát, sau ƒë√≥ ƒë∆∞·ª£c ensemble l·∫°i b·ªüi gating network.

### M·ª•c ti√™u ch√≠nh:
- **Diversity**: T·∫°o ra c√°c experts v·ªõi strengths kh√°c nhau
- **Complementarity**: Experts b·ªï sung cho nhau tr√™n different class groups
- **Calibration**: ƒê·∫£m b·∫£o outputs c√≥ confidence scores ƒë√°ng tin c·∫≠y
- **Robustness**: X·ª≠ l√Ω t·ªët long-tail distribution

## üèóÔ∏è Ki·∫øn tr√∫c Expert System

```
src/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îî‚îÄ‚îÄ train_expert.py        # Core training logic
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ experts.py            # Expert model wrapper
‚îÇ   ‚îú‚îÄ‚îÄ losses.py             # Specialized loss functions
‚îÇ   ‚îî‚îÄ‚îÄ backbones/
‚îÇ       ‚îî‚îÄ‚îÄ resnet_cifar.py   # CIFAR-optimized ResNet-32
‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îî‚îÄ‚îÄ calibration.py        # Temperature scaling
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ dataloader_utils.py   # Data loading utilities
```

## ü§ñ Three Expert Models

### 1. CE Expert (Cross Entropy Baseline)

```python
EXPERT_CONFIGS['ce'] = {
    'name': 'ce_baseline',
    'loss_type': 'ce', 
    'epochs': 256,
    'lr': 0.1,
    'weight_decay': 1e-4,
    'dropout_rate': 0.1,
    'milestones': [96, 192, 224],
    'gamma': 0.1
}
```

**ƒê·∫∑c ƒëi·ªÉm:**
- **Standard Cross Entropy**: Kh√¥ng c√≥ adjustments cho imbalance
- **Baseline Performance**: ƒê·∫°i di·ªán cho vanilla training
- **Strong on Head Classes**: Performs well tr√™n frequent classes
- **Use Case**: Provides reliable predictions cho head classes

### 2. LogitAdjust Expert

```python
EXPERT_CONFIGS['logitadjust'] = {
    'name': 'logitadjust_baseline',
    'loss_type': 'logitadjust',
    'epochs': 256, 
    'lr': 0.1,
    'weight_decay': 5e-4,
    'dropout_rate': 0.1,
    'milestones': [160, 180],
    'gamma': 0.1
}
```

**Loss Function:**
```python
class LogitAdjustLoss(nn.Module):
    def __init__(self, class_counts, tau=1.0):
        # Calculate class priors: œÄ_y = n_y / Œ£n_y  
        priors = class_counts / class_counts.sum()
        self.log_priors = torch.log(priors)
        self.tau = tau
    
    def forward(self, logits, target):
        # Adjust logits: ≈ù_y = s_y + œÑ * log(œÄ_y)
        adjusted_logits = logits + self.tau * self.log_priors
        return F.cross_entropy(adjusted_logits, target)
```

**Nguy√™n l√Ω:**
- **Prior Adjustment**: B√π tr·ª´ class imbalance b·∫±ng log priors
- **Tail Boost**: TƒÉng logits cho tail classes d·ª±a tr√™n frequency
- **Theoretical Foundation**: D·ª±a tr√™n Bayesian posterior adjustment
- **Balances Performance**: C·∫£i thi·ªán tail classes m√† kh√¥ng l√†m h·ªèng head classes

### 3. BalancedSoftmax Expert

```python
EXPERT_CONFIGS['balsoftmax'] = {
    'name': 'balsoftmax_baseline',
    'loss_type': 'balsoftmax',
    'epochs': 256,
    'lr': 0.1, 
    'weight_decay': 5e-4,
    'dropout_rate': 0.1,
    'milestones': [96, 192, 224],
    'gamma': 0.1
}
```

**Loss Function:**
```python
class BalancedSoftmaxLoss(nn.Module):
    def __init__(self, class_counts):
        # Use raw counts instead of priors
        self.log_priors = torch.log(class_counts)
    
    def forward(self, logits, target):
        # Adjust logits: ≈ù_y = s_y + log(n_y)
        adjusted_logits = logits + self.log_priors  
        return F.cross_entropy(adjusted_logits, target)
```

**Kh√°c bi·ªát v·ªõi LogitAdjust:**
- **Raw Counts**: S·ª≠ d·ª•ng log(n_y) thay v√¨ log(œÄ_y)
- **Stronger Adjustment**: T√°c ƒë·ªông m·∫°nh h∆°n l√™n rare classes
- **Different Balance Point**: Kh√°c bi·ªát trong c√°ch balance head/tail

## üèõÔ∏è Model Architecture Deep Dive

### Expert Model Wrapper

```python
class Expert(nn.Module):
    def __init__(self, num_classes=100, backbone_name='cifar_resnet32', 
                 dropout_rate=0.0, init_weights=True):
        self.backbone = CIFARResNet32(dropout_rate, init_weights)
        feature_dim = self.backbone.get_feature_dim()  # 64
        self.fc = nn.Linear(feature_dim, num_classes)
        
        # Temperature scaling for calibration
        self.temperature = nn.Parameter(torch.ones(1), requires_grad=False)
    
    def forward(self, x):
        features = self.backbone(x)
        logits = self.fc(features)
        return logits
    
    def get_calibrated_logits(self, x):
        return self.forward(x) / self.temperature
```

**Design Principles:**
- **Modular Design**: Backbone ri√™ng bi·ªát v·ªõi classifier
- **Calibration Ready**: Built-in temperature scaling
- **Feature Extraction**: C√≥ th·ªÉ extract features cho analysis
- **Flexible Architecture**: D·ªÖ thay ƒë·ªïi backbone n·∫øu c·∫ßn

### CIFAR-Optimized ResNet-32

```python
class CIFARResNet32:
    # Architecture: 1 + 3√ó(5√ó2) + 1 = 32 layers
    # 3 residual groups: [16, 32, 64] channels
    # 5 basic blocks per group
    # Output feature dim: 64
```

**Key Optimizations cho CIFAR:**

1. **Smaller Initial Conv**: 3x3 thay v√¨ 7x7 (ImageNet)
2. **No Initial MaxPool**: Gi·ªØ spatial resolution cho 32x32 images  
3. **Fewer Initial Channels**: 16 thay v√¨ 64
4. **Proper Downsampling**: Ch·ªâ downsample ·ªü group transitions
5. **Adaptive Global Pooling**: Flexibility cho different input sizes

```python
class CIFARResNet(nn.Module):
    def __init__(self, block, num_blocks, dropout_rate=0.0):
        # Initial: 32x32x3 ‚Üí 32x32x16
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        
        # Group 1: 32x32x16 ‚Üí 32x32x16 (5 blocks)
        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)
        
        # Group 2: 32x32x16 ‚Üí 16x16x32 (5 blocks)  
        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)
        
        # Group 3: 16x16x32 ‚Üí 8x8x64 (5 blocks)
        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)
        
        # Global pooling: 8x8x64 ‚Üí 1x1x64
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
```

## üîÑ Training Pipeline Deep Dive

### 1. Main Training Loop

```python
def train_single_expert(expert_key):
    """Train m·ªôt expert t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi"""
    
    # 1. Setup
    model = Expert(num_classes=100, dropout_rate=config['dropout_rate'])
    criterion = get_loss_function(config['loss_type'], train_loader)
    optimizer = optim.SGD(model.parameters(), lr=config['lr'], 
                         momentum=0.9, weight_decay=config['weight_decay'])
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, 
                                              milestones=config['milestones'], 
                                              gamma=config['gamma'])
    
    # 2. Training Loop
    for epoch in range(config['epochs']):
        # Train phase
        model.train()
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
        
        scheduler.step()
        
        # Validation phase
        val_acc, group_accs = validate_model(model, val_loader, device)
        
        # Save best model
        if val_acc > best_acc:
            torch.save(model.state_dict(), best_model_path)
    
    # 3. Post-processing
    # Temperature calibration
    scaler = TemperatureScaler()
    optimal_temp = scaler.fit(model, val_loader, device)
    model.set_temperature(optimal_temp)
    
    # Export logits for ensemble training
    export_logits_for_all_splits(model, expert_name)
```

### 2. Learning Rate Scheduling

**MultiStepLR Strategy:**
```python
# CE Expert: Conservative schedule
milestones = [96, 192, 224]  # 3/8, 3/4, 7/8 of total epochs
gamma = 0.1                   # LR decay factor

# LogitAdjust: Early decay for stability  
milestones = [160, 180]      # 5/8, 11/16 of total epochs
gamma = 0.1

# BalancedSoftmax: Same as CE
milestones = [96, 192, 224]
gamma = 0.1
```

**Rationale:**
- **Long Warm-up**: Nhi·ªÅu epochs ·ªü LR cao ƒë·ªÉ h·ªçc features
- **Gradual Decay**: T·ª´ t·ª´ gi·∫£m LR ƒë·ªÉ fine-tune
- **Final Convergence**: LR th·∫•p cu·ªëi ƒë·ªÉ converge t·ªët

### 3. Validation v·ªõi Group-wise Metrics

```python
def validate_model(model, val_loader, device):
    """Validation v·ªõi ph√¢n t√≠ch head/tail performance"""
    group_correct = {'head': 0, 'tail': 0}
    group_total = {'head': 0, 'tail': 0}
    
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            
            for i, target in enumerate(targets):
                pred = predicted[i]
                if target < 50:  # Head classes (0-49)
                    group_total['head'] += 1
                    if pred == target:
                        group_correct['head'] += 1 
                else:  # Tail classes (50-99)
                    group_total['tail'] += 1
                    if pred == target:
                        group_correct['tail'] += 1
    
    # Calculate accuracies
    overall_acc = 100 * correct / total
    group_accs = {
        'head': 100 * group_correct['head'] / group_total['head'],
        'tail': 100 * group_correct['tail'] / group_total['tail']
    }
    
    return overall_acc, group_accs
```

**Key Insights:**
- **Balanced Monitoring**: Theo d√µi c·∫£ head v√† tail performance
- **Early Detection**: Ph√°t hi·ªán s·ªõm overfitting tr√™n head classes
- **Model Selection**: Ch·ªçn model balance t·ªët head/tail

## üå°Ô∏è Temperature Calibration

### Why Calibration Matters

**Problem**: Raw neural network outputs kh√¥ng reflect true confidence
- High confidence predictions c√≥ th·ªÉ sai
- Low confidence predictions c√≥ th·ªÉ ƒë√∫ng  
- Imbalanced data l√†m tƒÉng miscalibration

**Solution**: Temperature scaling ƒë·ªÉ adjust confidence

### Temperature Scaling Algorithm

```python
class TemperatureScaler:
    def fit(self, model, dataloader, device):
        """Find optimal temperature T minimizing NLL"""
        
        # 1. Collect all logits and labels
        all_logits, all_labels = [], []
        with torch.no_grad():
            for inputs, labels in dataloader:
                logits = model(inputs.to(device))
                all_logits.append(logits)
                all_labels.append(labels.to(device))
        
        all_logits = torch.cat(all_logits)
        all_labels = torch.cat(all_labels)
        
        # 2. Optimize temperature parameter
        temperature = nn.Parameter(torch.ones(1).to(device))
        optimizer = optim.LBFGS([temperature], lr=0.01, max_iter=50)
        
        def closure():
            optimizer.zero_grad()
            # Scaled logits: s/T
            loss = F.cross_entropy(all_logits / temperature, all_labels)
            loss.backward()
            return loss
        
        optimizer.step(closure)
        return temperature.item()
```

**How it Works:**
1. **Collect Validation Data**: Get logits + labels t·ª´ validation set
2. **Optimize Temperature**: Minimize NLL w.r.t. T using L-BFGS  
3. **Apply Scaling**: Use T ƒë·ªÉ scale logits trong inference
4. **Calibrated Probabilities**: softmax(logits/T) gives better confidence

### Expected Calibration Error (ECE)

```python
def calculate_ece(posteriors, labels, n_bins=15):
    """Measure calibration quality"""
    confidences, predictions = torch.max(posteriors, 1)
    accuracies = predictions.eq(labels)
    
    # Bin predictions by confidence
    bin_boundaries = torch.linspace(0, 1, n_bins + 1)
    ece = 0.0
    
    for i in range(n_bins):
        in_bin = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i+1])
        prop_in_bin = in_bin.float().mean()
        
        if prop_in_bin > 0:
            accuracy_in_bin = accuracies[in_bin].float().mean()
            avg_confidence_in_bin = confidences[in_bin].mean()
            # |confidence - accuracy| weighted by bin size
            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
    
    return ece.item()
```

## üìä Logit Export System

### Purpose c·ªßa Logit Export

Sau khi training, m·ªói expert export **calibrated logits** cho t·∫•t c·∫£ splits:

```python
def export_logits_for_all_splits(model, expert_name):
    """Export logits cho t·∫•t c·∫£ dataset splits"""
    
    splits_to_export = [
        'train',      # Training data  
        'val_lt',     # Validation long-tail
        'test_lt',    # Test long-tail
        'tuneV',      # Calibration split
    ]
    
    for split_name in splits_to_export:
        # Load split indices
        with open(f"{split_name}_indices.json") as f:
            indices = json.load(f)
        
        # Create dataloader
        dataset = Subset(base_cifar_dataset, indices)
        loader = DataLoader(dataset, batch_size=512, shuffle=False)
        
        # Export calibrated logits
        all_logits = []
        with torch.no_grad():
            for inputs, _ in loader:
                logits = model.get_calibrated_logits(inputs.to(device))
                all_logits.append(logits.cpu())
        
        # Save as compressed tensors
        all_logits = torch.cat(all_logits)
        torch.save(all_logits.to(torch.float16), 
                  f"{output_dir}/{split_name}_logits.pt")
```

**Benefits:**
- **Efficiency**: Tr√°nh recompute logits trong ensemble training
- **Consistency**: ƒê·∫£m b·∫£o same preprocessing cho all experts  
- **Storage Optimization**: float16 ƒë·ªÉ ti·∫øt ki·ªám space
- **Easy Loading**: Simple torch.load() trong ensemble phase

### Output Structure

```
outputs/logits/cifar100_lt_if100/
‚îú‚îÄ‚îÄ ce_baseline/
‚îÇ   ‚îú‚îÄ‚îÄ train_logits.pt      # [~23K, 100] 
‚îÇ   ‚îú‚îÄ‚îÄ val_lt_logits.pt     # [~2.4K, 100]
‚îÇ   ‚îú‚îÄ‚îÄ test_lt_logits.pt    # [~10K, 100] 
‚îÇ   ‚îî‚îÄ‚îÄ tuneV_logits.pt      # [~1.8K, 100]
‚îú‚îÄ‚îÄ logitadjust_baseline/
‚îÇ   ‚îî‚îÄ‚îÄ ... (same structure)
‚îî‚îÄ‚îÄ balsoftmax_baseline/
    ‚îî‚îÄ‚îÄ ... (same structure)
```

## üéÆ Training Script Interface

### Main Script: `train_experts.py`

```bash
# Train all experts
python train_experts.py

# Train specific expert  
python train_experts.py --expert ce --verbose

# Override parameters
python train_experts.py --expert logitadjust --epochs 200 --lr 0.05

# Dry run to check configuration
python train_experts.py --dry-run --verbose
```

**Arguments:**
- `--expert {ce|logitadjust|balsoftmax|all}`: Which expert to train
- `--epochs INT`: Override number of training epochs
- `--lr FLOAT`: Override learning rate
- `--batch-size INT`: Override batch size  
- `--device {cpu|cuda|auto}`: Training device
- `--verbose`: Detailed output
- `--dry-run`: Show config without training
- `--resume`: Resume from checkpoint

### Configuration Overrides

```python
def apply_overrides(expert_configs, args):
    """Apply command-line overrides"""
    for expert_key in expert_configs:
        if args.epochs:
            expert_configs[expert_key]['epochs'] = args.epochs
        if args.lr: 
            expert_configs[expert_key]['lr'] = args.lr
        # ... other overrides
    
    return expert_configs
```

## üìà Performance Analysis

### Typical Training Results

**CE Expert:**
```
Epoch 256: Loss=1.234, Val Acc=62.5%, Head=78.2%, Tail=34.1%
Temperature calibration: T = 1.087
Final Results - Overall: 62.8%, Head: 78.5%, Tail: 34.7%
```

**LogitAdjust Expert:**  
```
Epoch 256: Loss=1.456, Val Acc=58.3%, Head=71.6%, Tail=41.2%  
Temperature calibration: T = 1.145
Final Results - Overall: 58.7%, Head: 72.1%, Tail: 42.0%
```

**BalancedSoftmax Expert:**
```
Epoch 256: Loss=1.389, Val Acc=59.8%, Head=73.4%, Tail=39.8%
Temperature calibration: T = 1.098  
Final Results - Overall: 60.1%, Head: 73.8%, Tail: 40.5%
```

### Performance Patterns

1. **CE Expert**: Highest overall accuracy, strong on head, weak on tail
2. **LogitAdjust**: Best tail performance, moderate overall accuracy  
3. **BalancedSoftmax**: Balanced between CE v√† LogitAdjust

**Complementarity**: M·ªói expert c√≥ strengths kh√°c nhau ‚Üí ensemble potential

## üîß Advanced Training Features

### 1. Automatic Mixed Precision (AMP)

```python
# Optional AMP support for faster training
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 2. Gradient Clipping

```python
# Prevent exploding gradients
max_grad_norm = 1.0
torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
```

### 3. Model Checkpointing

```python
# Save training state for resumability
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'best_acc': best_acc,
    'config': expert_config
}
torch.save(checkpoint, checkpoint_path)
```

## üß™ Usage Examples

### 1. Basic Training

```python
# Train all experts sequentially  
from train_experts import main
main()

# Or programmatically
from src.train.train_expert import train_single_expert

for expert in ['ce', 'logitadjust', 'balsoftmax']:
    model_path = train_single_expert(expert)
    print(f"Trained {expert}: {model_path}")
```

### 2. Custom Configuration

```python
from src.train.train_expert import EXPERT_CONFIGS

# Modify config before training
EXPERT_CONFIGS['ce']['epochs'] = 200
EXPERT_CONFIGS['ce']['lr'] = 0.05

# Add custom expert
EXPERT_CONFIGS['custom'] = {
    'name': 'custom_expert',
    'loss_type': 'ce', 
    'epochs': 100,
    'lr': 0.1,
    'weight_decay': 1e-3,
    'dropout_rate': 0.2,
    'milestones': [50, 80],
    'gamma': 0.1
}
```

### 3. Evaluation v√† Analysis

```python
# Load trained expert
from src.models.experts import Expert

model = Expert(num_classes=100)
model.load_state_dict(torch.load('checkpoints/experts/cifar100_lt_if100/ce_baseline.pth'))

# Evaluate on test set
from src.data.dataloader_utils import get_expert_training_dataloaders
_, test_loader = get_expert_training_dataloaders()

acc, group_accs = validate_model(model, test_loader, 'cuda')
print(f"Test Accuracy: {acc:.2f}%")
print(f"Head: {group_accs['head']:.2f}%, Tail: {group_accs['tail']:.2f}%")
```

## üîç Debugging & Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```python
# Solutions:
- Reduce batch_size in CONFIG['train_params']
- Use gradient accumulation
- Enable gradient checkpointing
- Use mixed precision training
```

**2. Poor Tail Performance**
```python
# Check:
- Class count distribution in dataset
- Loss function implementation 
- Learning rate too high/low
- Weight decay too strong
```

**3. Training Instability**
```python
# Solutions:
- Add gradient clipping
- Lower learning rate
- Increase warmup epochs
- Check data preprocessing
```

### Validation Checks

```python
def validate_expert_training():
    """Comprehensive validation of expert training setup"""
    
    # 1. Check data splits exist
    assert Path("data/cifar100_lt_if100_splits").exists()
    
    # 2. Check CUDA availability
    assert torch.cuda.is_available(), "CUDA required for training"
    
    # 3. Validate configs
    for expert, config in EXPERT_CONFIGS.items():
        assert config['epochs'] > 0
        assert 0 < config['lr'] < 1
        assert len(config['milestones']) > 0
    
    # 4. Test data loading
    train_loader, val_loader = get_dataloaders()
    batch = next(iter(train_loader))
    assert batch[0].shape[0] > 0  # Non-empty batch
    
    print("‚úÖ All validation checks passed!")
```

## üìä Expected Results

### Training Time (on RTX 3090)

- **CE Expert**: ~2.5 hours (256 epochs)
- **LogitAdjust Expert**: ~2.5 hours  
- **BalancedSoftmax Expert**: ~2.5 hours
- **Total**: ~7.5 hours cho all experts

### Model Sizes

- **Each Expert**: ~11.2M parameters
- **Checkpoint Size**: ~43MB per expert
- **Total Storage**: ~130MB cho all experts + logits

### Memory Requirements

- **Training**: ~8GB GPU memory (batch_size=128)
- **Inference**: ~2GB GPU memory  
- **Logit Storage**: ~500MB cho all splits

---

## ÔøΩ Expert Evaluation System

### 1. Individual Expert Performance Metrics

```python
def evaluate_single_expert(model, test_loader, class_to_group, num_groups):
    """
    Comprehensive evaluation c·ªßa t·ª´ng expert.
    
    Metrics:
    - Overall Accuracy: Standard classification accuracy
    - Per-Group Accuracy: Head vs Tail performance
    - Balanced Accuracy: (1/K) Œ£ acc_k
    - Worst-Group Accuracy: min_k acc_k  
    - Per-Class Accuracy: Detailed class-level analysis
    - Calibration Error (ECE): Before v√† after temperature scaling
    """
    
    model.eval()
    all_preds = []
    all_labels = []
    all_confidences = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            outputs = model(data.to(device))
            probs = torch.softmax(outputs, dim=1)
            preds = outputs.argmax(dim=1)
            confidences = probs.max(dim=1)[0]
            
            all_preds.append(preds.cpu())
            all_labels.append(labels)
            all_confidences.append(confidences.cpu())
    
    # Aggregate results
    preds = torch.cat(all_preds)
    labels = torch.cat(all_labels)
    confidences = torch.cat(all_confidences)
    
    # Overall metrics
    overall_acc = (preds == labels).float().mean().item()
    
    # Per-group analysis
    y_groups = class_to_group[labels]
    group_accuracies = []
    
    for k in range(num_groups):
        group_mask = (y_groups == k)
        if group_mask.sum() > 0:
            group_acc = (preds[group_mask] == labels[group_mask]).float().mean().item()
            group_accuracies.append(group_acc)
        else:
            group_accuracies.append(0.0)
    
    balanced_acc = np.mean(group_accuracies)
    worst_acc = np.min(group_accuracies)
    
    return {
        'overall_accuracy': overall_acc,
        'group_accuracies': group_accuracies,
        'balanced_accuracy': balanced_acc,
        'worst_accuracy': worst_acc,
        'head_accuracy': group_accuracies[0],
        'tail_accuracy': group_accuracies[1]
    }
```

### 2. Comparative Expert Analysis

```python
def compare_experts(expert_results):
    """
    So s√°nh performance c·ªßa 3 experts.
    
    Generates:
    - Performance comparison table
    - Strengths/weaknesses analysis
    - Complementarity assessment
    - Ensemble potential prediction
    """
    
    print("üìä EXPERT COMPARISON ANALYSIS")
    print("=" * 60)
    
    metrics = ['overall_accuracy', 'balanced_accuracy', 'head_accuracy', 'tail_accuracy']
    
    for metric in metrics:
        print(f"\n{metric.replace('_', ' ').title()}:")
        for expert_name, results in expert_results.items():
            value = results[metric]
            print(f"  {expert_name:>15}: {value:.4f}")
    
    # Find best performer per metric
    print(f"\nüèÜ Best Performers:")
    for metric in metrics:
        best_expert = max(expert_results.items(), key=lambda x: x[1][metric])
        print(f"  {metric:>20}: {best_expert[0]} ({best_expert[1][metric]:.4f})")
    
    # Complementarity analysis
    print(f"\nüîÑ Complementarity Analysis:")
    
    # Check if different experts excel on different groups
    head_best = max(expert_results.items(), key=lambda x: x[1]['head_accuracy'])
    tail_best = max(expert_results.items(), key=lambda x: x[1]['tail_accuracy'])
    
    print(f"  Head Classes Champion: {head_best[0]} ({head_best[1]['head_accuracy']:.4f})")
    print(f"  Tail Classes Champion: {tail_best[0]} ({tail_best[1]['tail_accuracy']:.4f})")
    
    if head_best[0] != tail_best[0]:
        print("  ‚úÖ Good complementarity detected - different experts excel on different groups!")
    else:
        print("  ‚ö†Ô∏è  Limited complementarity - same expert dominates both groups")
```

### 3. Expected Expert Performance (CIFAR-100-LT IF=100)

```python
EXPECTED_EXPERT_RESULTS = {
    'ce_baseline': {
        'overall_accuracy': 0.52-0.56,      # 52-56% overall
        'head_accuracy': 0.78-0.83,         # 78-83% on head classes  
        'tail_accuracy': 0.35-0.42,         # 35-42% on tail classes
        'balanced_accuracy': 0.56-0.62,     # 56-62% balanced
        'calibration_ece': 0.08-0.15        # Before temperature scaling
    },
    'logitadjust_baseline': {
        'overall_accuracy': 0.48-0.52,      # Slightly lower overall
        'head_accuracy': 0.72-0.78,         # Lower on head (by design)
        'tail_accuracy': 0.42-0.48,         # Higher on tail ‚úì
        'balanced_accuracy': 0.57-0.63,     # Better balanced ‚úì
        'calibration_ece': 0.06-0.12        # Often better calibrated
    },
    'balsoftmax_baseline': {
        'overall_accuracy': 0.49-0.53,      # Middle ground
        'head_accuracy': 0.74-0.80,         # Balanced approach
        'tail_accuracy': 0.40-0.46,         # Good tail performance
        'balanced_accuracy': 0.57-0.63,     # Consistent balanced
        'calibration_ece': 0.07-0.13        # Good calibration
    }
}
```

### 4. Calibration Assessment

```python
def assess_expert_calibration(expert_logits, labels, expert_name):
    """
    Evaluate calibration quality before v√† after temperature scaling.
    
    Steps:
    1. Compute ECE on raw logits
    2. Apply temperature scaling
    3. Compute ECE on calibrated logits  
    4. Generate reliability diagram
    5. Assess improvement
    """
    
    from src.metrics.calibration import calculate_ece, temperature_scale
    
    # Raw calibration
    raw_probs = torch.softmax(expert_logits, dim=1)
    raw_confidences = raw_probs.max(dim=1)[0]
    raw_predictions = expert_logits.argmax(dim=1)
    
    raw_ece = calculate_ece(raw_confidences, raw_predictions, labels, n_bins=15)
    
    # Temperature scaling
    temperature = temperature_scale(expert_logits, labels)
    calibrated_logits = expert_logits / temperature
    calibrated_probs = torch.softmax(calibrated_logits, dim=1)
    calibrated_confidences = calibrated_probs.max(dim=1)[0]
    
    calibrated_ece = calculate_ece(calibrated_confidences, raw_predictions, labels, n_bins=15)
    
    improvement = (raw_ece - calibrated_ece) / raw_ece * 100
    
    print(f"\nüìè CALIBRATION ANALYSIS - {expert_name}")
    print(f"  Raw ECE: {raw_ece:.4f}")
    print(f"  Calibrated ECE: {calibrated_ece:.4f}")  
    print(f"  Temperature: {temperature:.3f}")
    print(f"  Improvement: {improvement:.1f}%")
    
    return {
        'raw_ece': raw_ece,
        'calibrated_ece': calibrated_ece,
        'temperature': temperature,
        'improvement_pct': improvement
    }
```

### 5. Expert Ensemble Potential Analysis

```python
def analyze_ensemble_potential(expert_logits_dict, labels, class_to_group):
    """
    Analyze potential benefit of ensembling experts.
    
    Methods:
    1. Simple Average Ensemble
    2. Oracle Ensemble (best expert per sample)
    3. Disagreement Analysis
    4. Correlation Analysis
    """
    
    # Simple average ensemble
    avg_logits = torch.stack(list(expert_logits_dict.values())).mean(dim=0)
    avg_preds = avg_logits.argmax(dim=1)
    avg_accuracy = (avg_preds == labels).float().mean().item()
    
    # Oracle ensemble (upper bound)
    oracle_correct = 0
    total_samples = len(labels)
    
    for i in range(total_samples):
        # Check if any expert got this sample correct
        sample_correct = False
        for expert_logits in expert_logits_dict.values():
            expert_pred = expert_logits[i].argmax().item()
            if expert_pred == labels[i].item():
                sample_correct = True
                break
        if sample_correct:
            oracle_correct += 1
    
    oracle_accuracy = oracle_correct / total_samples
    
    # Disagreement analysis
    expert_preds = torch.stack([logits.argmax(dim=1) for logits in expert_logits_dict.values()])
    
    # Samples where all experts agree
    unanimous = (expert_preds[0] == expert_preds[1]) & (expert_preds[1] == expert_preds[2])
    unanimous_correct = (expert_preds[0][unanimous] == labels[unanimous])
    
    # Samples where experts disagree
    disagreement = ~unanimous
    disagreement_rate = disagreement.float().mean().item()
    
    print(f"\nü§ù ENSEMBLE POTENTIAL ANALYSIS")
    print(f"  Average Ensemble Accuracy: {avg_accuracy:.4f}")
    print(f"  Oracle Ensemble Accuracy: {oracle_accuracy:.4f}")
    print(f"  Improvement Potential: {(oracle_accuracy - avg_accuracy):.4f}")
    print(f"  Expert Disagreement Rate: {disagreement_rate:.3f}")
    print(f"  Unanimous Accuracy: {unanimous_correct.float().mean():.4f}")
    
    return {
        'average_ensemble_acc': avg_accuracy,
        'oracle_ensemble_acc': oracle_accuracy,
        'improvement_potential': oracle_accuracy - avg_accuracy,
        'disagreement_rate': disagreement_rate
    }
```

### 6. Expert Training Validation

```python
def validate_expert_training():
    """
    Post-training validation checks.
    
    Validates:
    1. Model convergence (loss plateauing)
    2. No overfitting (train vs val gap)
    3. Reasonable calibration
    4. Expected performance ranges
    5. Diversity across experts
    """
    
    print("üîç EXPERT TRAINING VALIDATION")
    print("=" * 50)
    
    expert_names = ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']
    validation_results = {}
    
    for expert_name in expert_names:
        checkpoint_path = f"./checkpoints/experts/cifar100_lt_if100/final_calibrated_{expert_name}.pth"
        
        # Load checkpoint
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # Check convergence
        train_losses = checkpoint.get('train_losses', [])
        val_losses = checkpoint.get('val_losses', [])
        
        if len(train_losses) > 10:
            final_train_loss = np.mean(train_losses[-10:])  # Last 10 epochs
            final_val_loss = np.mean(val_losses[-10:])
            overfitting_gap = final_val_loss - final_train_loss
            
            # Check if loss stabilized
            loss_std = np.std(train_losses[-20:])  # Stability in last 20 epochs
            converged = loss_std < 0.05  # Threshold for convergence
            
        else:
            converged = False
            overfitting_gap = float('inf')
        
        # Performance check
        test_acc = checkpoint.get('test_accuracy', 0.0)
        expected_range = EXPECTED_EXPERT_RESULTS[expert_name]['overall_accuracy']
        
        if isinstance(expected_range, (list, tuple)) and len(expected_range) == 2:
            performance_ok = expected_range[0] <= test_acc <= expected_range[1]
        else:
            performance_ok = True  # Skip if no expected range
        
        validation_results[expert_name] = {
            'converged': converged,
            'overfitting_gap': overfitting_gap,
            'performance_ok': performance_ok,
            'test_accuracy': test_acc
        }
        
        # Print results
        status = "‚úÖ" if converged and performance_ok and overfitting_gap < 0.2 else "‚ö†Ô∏è"
        print(f"{status} {expert_name}:")
        print(f"    Converged: {converged} (loss_std: {loss_std:.3f})")
        print(f"    Overfitting gap: {overfitting_gap:.3f}")
        print(f"    Test accuracy: {test_acc:.3f}")
        print(f"    Performance in range: {performance_ok}")
    
    # Overall validation
    all_good = all(r['converged'] and r['performance_ok'] and r['overfitting_gap'] < 0.2 
                   for r in validation_results.values())
    
    if all_good:
        print("\n‚úÖ All experts passed validation checks!")
    else:
        print("\n‚ö†Ô∏è  Some experts may need attention - check training logs")
    
    return validation_results
```

---

## ÔøΩüìù Summary

Expert training system trong AR-GSE ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ:

1. **Create Diversity**: 3 experts v·ªõi specialized strengths
2. **Handle Imbalance**: Specialized loss functions cho long-tail data
3. **Ensure Calibration**: Temperature scaling cho reliable confidence
4. **Enable Ensemble**: Export logits cho efficient ensemble training
5. **Provide Flexibility**: Configurable architectures v√† hyperparameters
6. **Comprehensive Evaluation**: Multi-metric assessment v√† validation

**Evaluation framework** ƒë·∫£m b·∫£o:
- **Individual Performance**: Detailed metrics cho m·ªói expert
- **Comparative Analysis**: Understanding strengths/weaknesses
- **Calibration Quality**: Reliable uncertainty estimates
- **Ensemble Potential**: Assessment c·ªßa combination benefits
- **Training Validation**: Quality assurance checks

System n√†y cung c·∫•p foundation v·ªØng ch·∫Øc cho AR-GSE ensemble, v·ªõi m·ªói expert contributing unique capabilities ƒë·ªÉ handle different aspects c·ªßa imbalanced classification problem.