# AR-GSE Ensemble Training: Balance & Constrained Plugin Algorithms

## ðŸ“– Tá»•ng quan

ÄÃ¢y lÃ  giai Ä‘oáº¡n cuá»‘i cÃ¹ng vÃ  quan trá»ng nháº¥t cá»§a há»‡ thá»‘ng AR-GSE, thá»±c hiá»‡n tá»‘i Æ°u hÃ³a ensemble thÃ´ng qua 2 thuáº­t toÃ¡n plugin:

1. **GSE-Balanced Plugin** (`gse_balanced_plugin.py`): Tá»‘i Æ°u hÃ³a theo balanced/worst-case error
2. **GSE-Constrained Plugin** (`gse_constrained_plugin.py`): Tá»‘i Æ°u hÃ³a vá»›i rÃ ng buá»™c cÃ´ng báº±ng (fairness constraints)

Cáº£ hai thuáº­t toÃ¡n Ä‘á»u dá»±a trÃªn **primal-dual optimization** vÃ  **fixed-point updates** Ä‘á»ƒ tÃ¬m tham sá»‘ tá»‘i Æ°u cho ensemble.

---

## ðŸŽ¯ Má»¥c tiÃªu chÃ­nh

### Input chÃ­nh:
- **Expert logits** Ä‘Ã£ Ä‘Æ°á»£c train vÃ  calibrated
- **Pre-trained gating network** tá»« giai Ä‘oáº¡n trÆ°á»›c
- **Data splits**: tuneV (S1) vÃ  val_lt (S2)

### Output má»¥c tiÃªu:
- **Î±* (alpha)**: per-group scaling parameters [K,]
- **Î¼* (mu)**: per-group bias parameters [K,]
- **t* (threshold)**: decision threshold (scalar hoáº·c per-group)
- **Optimized ensemble model** vá»›i hiá»‡u suáº¥t tá»‘i Æ°u

### Metrics tá»‘i Æ°u:
- **Balanced Error**: (1/K) Î£ e_k - lá»—i trung bÃ¬nh cÃ¡c nhÃ³m
- **Worst-case Error**: max_k e_k - lá»—i tá»‡ nháº¥t
- **Coverage**: tá»· lá»‡ samples Ä‘Æ°á»£c cháº¥p nháº­n
- **Fairness**: cÃ´ng báº±ng giá»¯a cÃ¡c nhÃ³m

---

## ðŸ”„ GSE-Balanced Plugin Algorithm

### 1. Kiáº¿n trÃºc tá»•ng quan

```python
# File: gse_balanced_plugin.py
# Core optimization loop:
for outer_iter in range(T):
    1. Fit threshold t on S1 for current (Î±, Î¼)
    2. Compute errors e_k on S2
    3. Update Î± via fixed-point method
    4. Update Î¼ via grid search over Î»
    5. Apply EMA smoothing and projection
```

### 2. Biáº¿n Ä‘áº§u vÃ o chÃ­nh

```python
CONFIG = {
    'balanced_params': {
        'T': 50,                    # Outer iterations
        'objective': 'balanced',    # 'balanced', 'worst', or 'hybrid'
        'lambda_grid': [-2.0..2.0], # Grid for Î¼ optimization
        'alpha_min': 0.75,          # Î± constraints
        'alpha_max': 1.35,
        'ema_alpha': 0.7,          # EMA coefficient for smoothing
        'beta_floor': 0.85,        # Lower bound for Î² updates
        'patience': 8,             # Early stopping
        'convergence_tol': 1e-4,   # Convergence threshold
        'adaptive_grid': True,     # Expand grid if needed
        'hybrid_weight': 0.3,      # Weight for worst error in hybrid
    }
}
```

### 3. Input Data Flow

```python
def main():
    # 1. Load expert logits
    S1_loader, S2_loader = load_data_from_logits(CONFIG)
    # S1 (tuneV): ~2500 samples for threshold fitting  
    # S2 (val_lt): ~2500 samples for optimization
    
    # 2. Setup grouping
    class_counts = get_cifar100_lt_counts(imb_factor=100)
    class_to_group = get_class_to_group_by_threshold(class_counts, threshold=20)
    # Táº¡o 2 nhÃ³m: head (>20 samples) vs tail (â‰¤20 samples)
    
    # 3. Load pre-trained GSE model
    model = AR_GSE(num_experts=3, num_classes=100, num_groups=2, gating_dim=24)
    # Load gating weights tá»« giai Ä‘oáº¡n training trÆ°á»›c
    
    # 4. Cache mixture posteriors
    eta_S1, y_S1 = cache_eta_mix(model, S1_loader, class_to_group)
    eta_S2, y_S2 = cache_eta_mix(model, S2_loader, class_to_group)
    # Î·Ìƒ(x) = Î£_e w^(e)(x) * p^(e)(y|x): mixture posterior
```

### 4. Core Algorithm Components

#### A. Cache Mixture Posteriors
```python
@torch.no_grad()
def cache_eta_mix(gse_model, loader, class_to_group):
    """
    TÃ­nh mixture posterior Î·Ìƒ(x) tá»« expert outputs vÃ  gating weights.
    
    Input:
        - logits: [B, E, C] tá»« E=3 experts
        - gating_net: pre-trained gating network
    
    Output:
        - eta: [N, C] mixture posteriors
        - labels: [N] ground truth
    
    Formula:
        Î·Ìƒ_y(x) = Î£_e w^(e)(x) * p^(e)(y|x)
        where:
        - w^(e)(x): gating weights [B, E]
        - p^(e)(y|x): expert posteriors [B, E, C]
    """
```

#### B. Margin Computation
```python
def compute_raw_margin(eta, alpha, mu, class_to_group):
    """
    TÃ­nh raw margin score cho selective prediction.
    
    Formula:
        margin = max_y Î±_{g(y)} * Î·Ìƒ_y - Î£_y (1/Î±_{g(y)} - Î¼_{g(y)}) * Î·Ìƒ_y
    
    Input:
        - eta: [N, C] mixture posteriors
        - alpha: [K] per-group scaling
        - mu: [K] per-group bias
        
    Output:
        - raw_margin: [N] margin scores
    """
    score = (alpha[class_to_group] * eta).max(dim=1).values
    coeff = 1.0 / alpha[class_to_group] - mu[class_to_group]  
    threshold_term = (coeff.unsqueeze(0) * eta).sum(dim=1)
    return score - threshold_term
```

#### C. Error Computation
```python
def balanced_error_on_S(eta, y, alpha, mu, t, class_to_group, K):
    """
    TÃ­nh balanced error trÃªn split S.
    
    Steps:
    1. Compute margins vÃ  acceptance
    2. TÃ­nh per-group error rates
    3. Return balanced error = (1/K) Î£ e_k
    
    Formula:
        e_k = 1 - (correct_accepted_k / total_accepted_k)
        balanced_error = (1/K) Î£_k e_k
    """
```

#### D. Parameter Updates

##### Alpha Update (Fixed-Point Method)
```python
def update_alpha_fixed_point(eta_S1, y_S1, alpha, mu, t, class_to_group, K, config):
    """
    Cáº­p nháº­t Î± theo fixed-point iteration.
    
    Algorithm:
    1. Compute current acceptance rates per group
    2. Update: Î±_k^{new} = acceptance_rate_k + regularization
    3. Apply EMA smoothing: Î± = ema_alpha * Î±_old + (1-ema_alpha) * Î±_new
    4. Project to valid range [Î±_min, Î±_max]
    
    Input variables:
        - eta_S1: [N1, C] posteriors on tuning split
        - alpha: [K] current scaling parameters
        - mu: [K] current bias parameters
        
    Output:
        - alpha_new: [K] updated parameters
    """
    raw_margins = compute_raw_margin(eta_S1, alpha, mu, class_to_group)
    accepted = (raw_margins >= t)
    
    alpha_new = torch.zeros(K)
    for k in range(K):
        group_mask = (class_to_group[y_S1] == k)
        if group_mask.sum() > 0:
            acceptance_rate = (accepted & group_mask).sum().float() / group_mask.sum()
            alpha_new[k] = acceptance_rate + config['regularization']
    
    # EMA smoothing
    alpha = config['ema_alpha'] * alpha + (1 - config['ema_alpha']) * alpha_new
    return project_alpha(alpha, config['alpha_min'], config['alpha_max'])
```

##### Mu Update (Grid Search)
```python
def update_mu_grid_search(eta_S2, y_S2, alpha, t, class_to_group, K, config):
    """
    TÃ¬m Î¼ tá»‘i Æ°u thÃ´ng qua grid search.
    
    Algorithm:
    1. For each Î» in lambda_grid:
        - Set Î¼ = [+Î»/2, -Î»/2] (for K=2)
        - Compute objective on S2
        - Track best Î¼
    2. Return Î¼ with lowest objective
    
    Variables:
        - lambda_grid: [-2.0, -1.9, ..., 1.9, 2.0]
        - objective_type: 'balanced', 'worst', or 'hybrid'
    """
```

### 5. Objective Functions

```python
# Balanced Error (default)
objective = e_k.mean()

# Worst-case Error  
objective = e_k.max()

# Hybrid (weighted combination)
balanced_error = e_k.mean()
worst_error = e_k.max() 
objective = (1 - hybrid_weight) * balanced_error + hybrid_weight * worst_error
```

### 6. Output Results

```python
# Final optimized parameters
alpha_star = [1.0234, 0.8765]  # Per-group scaling
mu_star = [0.1234, -0.1234]    # Per-group bias  
t_star = 0.5678                # Decision threshold

# Performance metrics
balanced_error = 0.1234        # (1/K) Î£ e_k
worst_error = 0.2345          # max_k e_k
coverage = 0.8567             # Acceptance rate

# Checkpoints saved to:
# ./checkpoints/argse_balance/cifar100_lt_if100/gse_balanced_plugin.ckpt
```

---

## ðŸ”’ GSE-Constrained Plugin Algorithm

### 1. Kiáº¿n trÃºc Lagrangian Optimization

```python
# File: gse_constrained_plugin.py
# Lagrangian formulation:
L(Î±, Î¼, t, Î», Î½) = (1/K) Î£ e_k + Î»(Ï„ - (1/K) Î£ cov_k) + Î£ Î½_k(e_k - Î´)

# Where:
# - e_k: per-group error when accepting
# - cov_k: per-group coverage  
# - Ï„: minimum average coverage constraint (e.g., 0.65)
# - Î´: maximum per-group error constraint  
# - Î»: coverage multiplier (dual variable)
# - Î½_k: fairness multipliers per group (dual variables)
```

### 2. Biáº¿n Ä‘áº§u vÃ o chÃ­nh

```python
CONFIG = {
    'constrained_params': {
        'T': 50,                   # Outer iterations
        'tau': 0.65,              # Coverage constraint: cov â‰¥ Ï„
        'delta_multiplier': 1.3,   # Î´ = delta_multiplier Ã— avg_error
        'eta_dual': 0.05,         # Dual step size
        'eta_primal': 0.01,       # Primal step size
        'lambda_grid': [-2.0..2.0], # Grid for primal updates
        'warmup_iters': 10,       # Warmup before enforcing constraints
        'adaptive_delta': True,    # Adaptively adjust Î´
        'patience': 8,            # Early stopping
    }
}
```

### 3. Core Algorithm Flow

```python
def gse_constrained_plugin(eta_S1, y_S1, eta_S2, y_S2, class_to_group, K, config):
    """
    Main constrained optimization vá»›i primal-dual updates.
    
    Input Variables:
        - eta_S1, y_S1: tuning split for threshold fitting
        - eta_S2, y_S2: validation split for optimization  
        - class_to_group: [C] mapping classes to groups
        - K: number of groups (typically 2)
        
    Algorithm Flow:
    1. Initialize primal (Î±, Î¼, t) and dual (Î», Î½) variables
    2. For each outer iteration:
        a. Fit threshold t on S1
        b. Compute metrics (e_k, cov_k) on S2  
        c. Update primal variables (Î±, Î¼) via grid search
        d. Update dual variables (Î», Î½) via gradient ascent
        e. Check convergence
        
    Output:
        - (Î±*, Î¼*, t*): optimal primal variables
        - history: optimization trajectory
    """
```

### 4. Constraint Handling

#### A. Coverage Constraint
```python
# Constraint: average coverage â‰¥ Ï„
coverage_constraint = tau - cov_k.mean()  # â‰¤ 0 when satisfied

# Dual update: Î» â† max(0, Î» + Î·_dual Ã— violation)
lambda_cov = max(0.0, lambda_cov + eta_dual * coverage_constraint)
```

#### B. Fairness Constraint  
```python
# Constraint: per-group error â‰¤ Î´
fairness_violations = e_k - delta  # â‰¤ 0 when satisfied

# Dual update: Î½_k â† max(0, Î½_k + Î·_dual Ã— violation_k)  
nu = torch.clamp(nu + eta_dual * fairness_violations, min=0.0)

# Adaptive Î´ adjustment
if adaptive_delta and current_avg_error > 0:
    delta = max(delta * 0.95, delta_multiplier * current_avg_error)
```

### 5. Primal-Dual Updates

#### Primal Update (Î±, Î¼)
```python
# Grid search over Î¼ candidates
for lam in lambda_grid:
    mu_candidate = torch.tensor([+lam/2, -lam/2])  # For K=2
    
    # Fixed-point updates for Î± given Î¼
    alpha_candidate = alpha.clone()
    for fp_iter in range(3):
        # Compute acceptance rates
        raw_margins = compute_raw_margin(eta_S1, alpha_candidate, mu_candidate, class_to_group)
        accepted = (raw_margins >= t)
        
        # Update Î± based on acceptance rates
        for k in range(K):
            group_mask = (class_to_group[y_S1] == k)
            acceptance_rate = (accepted & group_mask).sum() / group_mask.sum()
            alpha_new[k] = acceptance_rate + regularization
        
        # EMA smoothing
        alpha_candidate = 0.7 * alpha_candidate + 0.3 * alpha_new
        alpha_candidate = project_alpha(alpha_candidate)
    
    # Evaluate Lagrangian objective
    e_k, cov_k, _, _ = compute_group_metrics(eta_S2, y_S2, alpha_candidate, mu_candidate, t)
    
    lagrangian = e_k.mean()  # Primal objective
    if outer_iter >= warmup_iters:
        lagrangian += lambda_cov * (tau - cov_k.mean())  # Coverage penalty
        lagrangian += (nu * torch.clamp(e_k - delta, min=0)).sum()  # Fairness penalty
    
    # Track best candidate
    if lagrangian < best_lagrangian:
        best_alpha, best_mu = alpha_candidate, mu_candidate
```

#### Dual Update (Î», Î½)
```python
# Coverage multiplier update
lambda_cov = max(0.0, lambda_cov + eta_dual * coverage_violation)

# Per-group fairness multiplier update  
nu = torch.clamp(nu + eta_dual * fairness_violations, min=0.0)
```

### 6. Output Analysis

```python
# Final Results Example:
Î±* = [1.0145, 0.9823]     # Balanced scaling between groups
Î¼* = [0.0234, -0.0234]    # Small bias correction
t* = 0.4567               # Threshold for Ï„=0.65 coverage

# Metrics achieved:
balanced_error = 0.0987   # Primary objective
worst_error = 0.1234     # Worst group performance  
coverage = 0.653         # Satisfies Ï„ â‰¥ 0.65
per_group_errors = [0.0876, 0.1098]  # Both â‰¤ Î´
per_group_coverage = [0.645, 0.661]  # Balanced coverage

# Constraint satisfaction:
coverage_satisfied = True    # 0.653 â‰¥ 0.65 âœ“
fairness_satisfied = True   # All e_k â‰¤ Î´ âœ“
```

---

## ðŸ”„ So sÃ¡nh hai thuáº­t toÃ¡n

| Aspect | GSE-Balanced | GSE-Constrained |
|--------|--------------|-----------------|
| **Objective** | Minimize balanced/worst error | Minimize error subject to constraints |
| **Constraints** | None (unconstrained) | Coverage â‰¥ Ï„, Error â‰¤ Î´ |  
| **Optimization** | Direct minimization | Lagrangian dual method |
| **Complexity** | Simpler, faster | More complex, principled |
| **Use case** | General optimization | Fairness-aware deployment |
| **Parameters** | (Î±, Î¼, t) | (Î±, Î¼, t) + (Î», Î½) |

---

## ðŸ“Š Evaluation vÃ  Metrics

### 1. Performance Metrics
```python
def evaluate_final_performance(eta_S2, y_S2, alpha_star, mu_star, t_star, class_to_group, K):
    """
    ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t cuá»‘i cÃ¹ng trÃªn validation set.
    
    Computed Metrics:
    - Balanced Error: (1/K) Î£ e_k  
    - Worst-case Error: max_k e_k
    - Coverage: fraction of samples accepted
    - Per-group Error: e_k for each group k
    - Per-group Coverage: cov_k for each group k
    - Accuracy on accepted: correct predictions / accepted predictions
    """
```

### 2. Convergence Analysis
```python
# Tracking optimization history:
history = {
    'balanced_error': [...],     # Balanced error per iteration
    'worst_error': [...],        # Worst error per iteration  
    'coverage': [...],           # Coverage per iteration
    'lagrangian': [...],         # Objective per iteration (constrained only)
    'lambda_cov': [...],         # Coverage multiplier (constrained only)
    'nu': [...],                # Fairness multipliers (constrained only)
    'alpha': [...],             # Alpha evolution
    'mu': [...]                 # Mu evolution
}
```

---

## ðŸš€ Usage Examples

### 1. Cháº¡y GSE-Balanced Plugin
```bash
cd c:\Users\Admin\Documents\GitHub\AR-GSE
python -m src.train.gse_balanced_plugin

# Output:
# âœ… Loaded S1 (tuneV): 10 batches  
# âœ… Loaded S2 (val_lt): 10 batches
# âœ… Groups: 34 head classes, 66 tail classes
# âœ… Loaded pre-trained gating
# âœ… Cached Î·Ìƒ_S1: [2500, 100], y_S1: [2500]
# âœ… Cached Î·Ìƒ_S2: [2500, 100], y_S2: [2500]
# 
# === GSE Balanced Plugin ===
# Objective: balanced, T=50, EMA=0.70
# [1 ] bal=0.2134, worst=0.3456, obj=0.2134
# [2 ] bal=0.1987, worst=0.3201, obj=0.1987  
# ...
# [23] bal=0.0987, worst=0.1234, obj=0.0987
# Early stopping at iteration 23
# 
# Î±* = [1.0145, 0.9823]
# Î¼* = [0.0234, -0.0234]  
# t* = 0.4567
# ðŸ’¾ Saved checkpoint to ./checkpoints/argse_balance/cifar100_lt_if100/gse_balanced_plugin.ckpt
```

### 2. Cháº¡y GSE-Constrained Plugin
```bash 
python -m src.train.gse_constrained_plugin

# Output:
# === GSE Constrained Plugin ===
# Coverage constraint: Ï„ â‰¥ 0.65
# Outer iterations: 50
# Initial Î´ = 0.267 (1.3Ã— avg error)
# 
# [1 ] bal=0.2134, worst=0.3456, cov=0.580, L=0.2134
# [2 ] bal=0.1987, worst=0.3201, cov=0.623, L=0.1987
# ...  
# [15] bal=0.0987, worst=0.1234, cov=0.653, L=0.0956, Î»=0.234, Î½_max=0.123
# Early stopping at iteration 15
#
# âœ… Final Results:
# Î±* = [1.0145, 0.9823]
# Î¼* = [0.0234, -0.0234]
# Coverage = 0.653 (â‰¥ 0.65 âœ“)
# Per-group errors: [0.0876, 0.1098] (all â‰¤ 0.267 âœ“)
# ðŸ’¾ Saved checkpoint + plots
```

---

## ðŸ”§ Configuration tuning

### 1. Key hyperparameters

#### GSE-Balanced:
- `T`: Sá»‘ iterations (50 thÆ°á»ng Ä‘á»§)
- `ema_alpha`: EMA coefficient (0.7 cho stability)  
- `lambda_grid`: Range cho Î¼ optimization ([-2, 2])
- `alpha_min/max`: Constraints cho Î± ([0.75, 1.35])
- `objective`: 'balanced' vs 'worst' vs 'hybrid'

#### GSE-Constrained:
- `tau`: Coverage constraint (0.65 = 65% minimum)
- `delta_multiplier`: Fairness constraint (1.3Ã— avg error) 
- `eta_dual`: Dual step size (0.05 cho stability)
- `warmup_iters`: Iterations trÆ°á»›c khi Ã¡p dá»¥ng constraints (10)

### 2. Troubleshooting

#### Common issues:
1. **Convergence problems**: Giáº£m eta_dual/eta_primal
2. **Unstable Î± updates**: TÄƒng ema_alpha 
3. **Coverage too low**: Giáº£m tau hoáº·c tÄƒng delta_multiplier
4. **Poor fairness**: Äiá»u chá»‰nh lambda_grid range

---

## ðŸ“ File Structure

```
checkpoints/argse_balance/cifar100_lt_if100/
â”œâ”€â”€ gse_balanced_plugin.ckpt      # Balanced plugin results
â””â”€â”€ optimization_history.png      # Training curves

checkpoints/argse_constrained_plugin/cifar100_lt_if100/  
â”œâ”€â”€ gse_constrained_plugin.ckpt   # Constrained plugin results
â””â”€â”€ constrained_optimization_history.png  # Constraint satisfaction curves

outputs/logits/cifar100_lt_if100/  # Input expert logits
â”œâ”€â”€ ce_baseline/
â”œâ”€â”€ logitadjust_baseline/  
â””â”€â”€ balsoftmax_baseline/
```

---

## ðŸŽ¯ Summary

**GSE Ensemble Training** lÃ  giai Ä‘oáº¡n cuá»‘i cÃ¹ng káº¿t há»£p táº¥t cáº£ components cá»§a AR-GSE:

1. **Input**: Expert logits + Pre-trained gating + Data splits
2. **Process**: Plugin optimization vá»›i primal-dual methods  
3. **Output**: Optimal ensemble parameters (Î±*, Î¼*, t*)
4. **Result**: High-performance selective prediction system

Hai thuáº­t toÃ¡n plugin cung cáº¥p flexibility:
- **Balanced**: ÄÆ¡n giáº£n, tá»‘i Æ°u performance trá»±c tiáº¿p
- **Constrained**: Phá»©c táº¡p hÆ¡n nhÆ°ng Ä‘áº£m báº£o fairness constraints

Káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  má»™t AR-GSE model hoÃ n chá»‰nh cÃ³ thá»ƒ deploy cho selective prediction trÃªn CIFAR-100-LT vá»›i hiá»‡u suáº¥t cao vÃ  cÃ´ng báº±ng giá»¯a cÃ¡c nhÃ³m head/tail classes.