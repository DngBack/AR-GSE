# AR-GSE Ensemble Training: Balance & Constrained Plugin Algorithms

## üìñ T·ªïng quan

ƒê√¢y l√† giai ƒëo·∫°n cu·ªëi c√πng v√† quan tr·ªçng nh·∫•t c·ªßa h·ªá th·ªëng AR-GSE, th·ª±c hi·ªán t·ªëi ∆∞u h√≥a ensemble th√¥ng qua 2 thu·∫≠t to√°n plugin:

1. **GSE-Balanced Plugin** (`gse_balanced_plugin.py`): T·ªëi ∆∞u h√≥a theo balanced/worst-case error
2. **GSE-Constrained Plugin** (`gse_constrained_plugin.py`): T·ªëi ∆∞u h√≥a v·ªõi r√†ng bu·ªôc c√¥ng b·∫±ng (fairness constraints)

C·∫£ hai thu·∫≠t to√°n ƒë·ªÅu d·ª±a tr√™n **primal-dual optimization** v√† **fixed-point updates** ƒë·ªÉ t√¨m tham s·ªë t·ªëi ∆∞u cho ensemble.

---

## üéØ M·ª•c ti√™u ch√≠nh

### Input ch√≠nh:
- **Expert logits** ƒë√£ ƒë∆∞·ª£c train v√† calibrated
- **Pre-trained gating network** t·ª´ giai ƒëo·∫°n tr∆∞·ªõc
- **Data splits**: tuneV (S1) v√† val_lt (S2)

### Output m·ª•c ti√™u:
- **Œ±* (alpha)**: per-group scaling parameters [K,]
- **Œº* (mu)**: per-group bias parameters [K,]
- **t* (threshold)**: decision threshold (scalar ho·∫∑c per-group)
- **Optimized ensemble model** v·ªõi hi·ªáu su·∫•t t·ªëi ∆∞u

### Metrics t·ªëi ∆∞u:
- **Balanced Error**: (1/K) Œ£ e_k - l·ªói trung b√¨nh c√°c nh√≥m
- **Worst-case Error**: max_k e_k - l·ªói t·ªá nh·∫•t
- **Coverage**: t·ª∑ l·ªá samples ƒë∆∞·ª£c ch·∫•p nh·∫≠n
- **Fairness**: c√¥ng b·∫±ng gi·ªØa c√°c nh√≥m

---

## üîÑ GSE-Balanced Plugin Algorithm

### 1. Ki·∫øn tr√∫c t·ªïng quan

```python
# File: gse_balanced_plugin.py
# Core optimization loop:
for outer_iter in range(T):
    1. Fit threshold t on S1 for current (Œ±, Œº)
    2. Compute errors e_k on S2
    3. Update Œ± via fixed-point method
    4. Update Œº via grid search over Œª
    5. Apply EMA smoothing and projection
```

### 2. Bi·∫øn ƒë·∫ßu v√†o ch√≠nh

```python
CONFIG = {
    'balanced_params': {
        'T': 50,                    # Outer iterations
        'objective': 'balanced',    # 'balanced', 'worst', or 'hybrid'
        'lambda_grid': [-2.0..2.0], # Grid for Œº optimization
        'alpha_min': 0.75,          # Œ± constraints
        'alpha_max': 1.35,
        'ema_alpha': 0.7,          # EMA coefficient for smoothing
        'beta_floor': 0.85,        # Lower bound for Œ≤ updates
        'patience': 8,             # Early stopping
        'convergence_tol': 1e-4,   # Convergence threshold
        'adaptive_grid': True,     # Expand grid if needed
        'hybrid_weight': 0.3,      # Weight for worst error in hybrid
    }
}
```

### 3. Input Data Flow

```python
def main():
    # 1. Load expert logits
    S1_loader, S2_loader = load_data_from_logits(CONFIG)
    # S1 (tuneV): ~2500 samples for threshold fitting  
    # S2 (val_lt): ~2500 samples for optimization
    
    # 2. Setup grouping
    class_counts = get_cifar100_lt_counts(imb_factor=100)
    class_to_group = get_class_to_group_by_threshold(class_counts, threshold=20)
    # T·∫°o 2 nh√≥m: head (>20 samples) vs tail (‚â§20 samples)
    
    # 3. Load pre-trained GSE model
    model = AR_GSE(num_experts=3, num_classes=100, num_groups=2, gating_dim=24)
    # Load gating weights t·ª´ giai ƒëo·∫°n training tr∆∞·ªõc
    
    # 4. Cache mixture posteriors
    eta_S1, y_S1 = cache_eta_mix(model, S1_loader, class_to_group)
    eta_S2, y_S2 = cache_eta_mix(model, S2_loader, class_to_group)
    # Œ∑ÃÉ(x) = Œ£_e w^(e)(x) * p^(e)(y|x): mixture posterior
```

### 4. Core Algorithm Components

#### A. Cache Mixture Posteriors
```python
@torch.no_grad()
def cache_eta_mix(gse_model, loader, class_to_group):
    """
    T√≠nh mixture posterior Œ∑ÃÉ(x) t·ª´ expert outputs v√† gating weights.
    
    Input:
        - logits: [B, E, C] t·ª´ E=3 experts
        - gating_net: pre-trained gating network
    
    Output:
        - eta: [N, C] mixture posteriors
        - labels: [N] ground truth
    
    Formula:
        Œ∑ÃÉ_y(x) = Œ£_e w^(e)(x) * p^(e)(y|x)
        where:
        - w^(e)(x): gating weights [B, E]
        - p^(e)(y|x): expert posteriors [B, E, C]
    """
```

#### B. Margin Computation
```python
def compute_raw_margin(eta, alpha, mu, class_to_group):
    """
    T√≠nh raw margin score cho selective prediction.
    
    Formula:
        margin = max_y Œ±_{g(y)} * Œ∑ÃÉ_y - Œ£_y (1/Œ±_{g(y)} - Œº_{g(y)}) * Œ∑ÃÉ_y
    
    Input:
        - eta: [N, C] mixture posteriors
        - alpha: [K] per-group scaling
        - mu: [K] per-group bias
        
    Output:
        - raw_margin: [N] margin scores
    """
    score = (alpha[class_to_group] * eta).max(dim=1).values
    coeff = 1.0 / alpha[class_to_group] - mu[class_to_group]  
    threshold_term = (coeff.unsqueeze(0) * eta).sum(dim=1)
    return score - threshold_term
```

#### C. Error Computation
```python
def balanced_error_on_S(eta, y, alpha, mu, t, class_to_group, K):
    """
    T√≠nh balanced error tr√™n split S.
    
    Steps:
    1. Compute margins v√† acceptance
    2. T√≠nh per-group error rates
    3. Return balanced error = (1/K) Œ£ e_k
    
    Formula:
        e_k = 1 - (correct_accepted_k / total_accepted_k)
        balanced_error = (1/K) Œ£_k e_k
    """
```

#### D. Parameter Updates

##### Alpha Update (Fixed-Point Method)
```python
def update_alpha_fixed_point(eta_S1, y_S1, alpha, mu, t, class_to_group, K, config):
    """
    C·∫≠p nh·∫≠t Œ± theo fixed-point iteration.
    
    Algorithm:
    1. Compute current acceptance rates per group
    2. Update: Œ±_k^{new} = acceptance_rate_k + regularization
    3. Apply EMA smoothing: Œ± = ema_alpha * Œ±_old + (1-ema_alpha) * Œ±_new
    4. Project to valid range [Œ±_min, Œ±_max]
    
    Input variables:
        - eta_S1: [N1, C] posteriors on tuning split
        - alpha: [K] current scaling parameters
        - mu: [K] current bias parameters
        
    Output:
        - alpha_new: [K] updated parameters
    """
    raw_margins = compute_raw_margin(eta_S1, alpha, mu, class_to_group)
    accepted = (raw_margins >= t)
    
    alpha_new = torch.zeros(K)
    for k in range(K):
        group_mask = (class_to_group[y_S1] == k)
        if group_mask.sum() > 0:
            acceptance_rate = (accepted & group_mask).sum().float() / group_mask.sum()
            alpha_new[k] = acceptance_rate + config['regularization']
    
    # EMA smoothing
    alpha = config['ema_alpha'] * alpha + (1 - config['ema_alpha']) * alpha_new
    return project_alpha(alpha, config['alpha_min'], config['alpha_max'])
```

##### Mu Update (Grid Search)
```python
def update_mu_grid_search(eta_S2, y_S2, alpha, t, class_to_group, K, config):
    """
    T√¨m Œº t·ªëi ∆∞u th√¥ng qua grid search.
    
    Algorithm:
    1. For each Œª in lambda_grid:
        - Set Œº = [+Œª/2, -Œª/2] (for K=2)
        - Compute objective on S2
        - Track best Œº
    2. Return Œº with lowest objective
    
    Variables:
        - lambda_grid: [-2.0, -1.9, ..., 1.9, 2.0]
        - objective_type: 'balanced', 'worst', or 'hybrid'
    """
```

### 5. Objective Functions

```python
# Balanced Error (default)
objective = e_k.mean()

# Worst-case Error  
objective = e_k.max()

# Hybrid (weighted combination)
balanced_error = e_k.mean()
worst_error = e_k.max() 
objective = (1 - hybrid_weight) * balanced_error + hybrid_weight * worst_error
```

### 6. Output Results

```python
# Final optimized parameters
alpha_star = [1.0234, 0.8765]  # Per-group scaling
mu_star = [0.1234, -0.1234]    # Per-group bias  
t_star = 0.5678                # Decision threshold

# Performance metrics
balanced_error = 0.1234        # (1/K) Œ£ e_k
worst_error = 0.2345          # max_k e_k
coverage = 0.8567             # Acceptance rate

# Checkpoints saved to:
# ./checkpoints/argse_balance/cifar100_lt_if100/gse_balanced_plugin.ckpt
```

---

## üîí GSE-Constrained Plugin Algorithm

### 1. Ki·∫øn tr√∫c Lagrangian Optimization

```python
# File: gse_constrained_plugin.py
# Lagrangian formulation:
L(Œ±, Œº, t, Œª, ŒΩ) = (1/K) Œ£ e_k + Œª(œÑ - (1/K) Œ£ cov_k) + Œ£ ŒΩ_k(e_k - Œ¥)

# Where:
# - e_k: per-group error when accepting
# - cov_k: per-group coverage  
# - œÑ: minimum average coverage constraint (e.g., 0.65)
# - Œ¥: maximum per-group error constraint  
# - Œª: coverage multiplier (dual variable)
# - ŒΩ_k: fairness multipliers per group (dual variables)
```

### 2. Bi·∫øn ƒë·∫ßu v√†o ch√≠nh

```python
CONFIG = {
    'constrained_params': {
        'T': 50,                   # Outer iterations
        'tau': 0.65,              # Coverage constraint: cov ‚â• œÑ
        'delta_multiplier': 1.3,   # Œ¥ = delta_multiplier √ó avg_error
        'eta_dual': 0.05,         # Dual step size
        'eta_primal': 0.01,       # Primal step size
        'lambda_grid': [-2.0..2.0], # Grid for primal updates
        'warmup_iters': 10,       # Warmup before enforcing constraints
        'adaptive_delta': True,    # Adaptively adjust Œ¥
        'patience': 8,            # Early stopping
    }
}
```

### 3. Core Algorithm Flow

```python
def gse_constrained_plugin(eta_S1, y_S1, eta_S2, y_S2, class_to_group, K, config):
    """
    Main constrained optimization v·ªõi primal-dual updates.
    
    Input Variables:
        - eta_S1, y_S1: tuning split for threshold fitting
        - eta_S2, y_S2: validation split for optimization  
        - class_to_group: [C] mapping classes to groups
        - K: number of groups (typically 2)
        
    Algorithm Flow:
    1. Initialize primal (Œ±, Œº, t) and dual (Œª, ŒΩ) variables
    2. For each outer iteration:
        a. Fit threshold t on S1
        b. Compute metrics (e_k, cov_k) on S2  
        c. Update primal variables (Œ±, Œº) via grid search
        d. Update dual variables (Œª, ŒΩ) via gradient ascent
        e. Check convergence
        
    Output:
        - (Œ±*, Œº*, t*): optimal primal variables
        - history: optimization trajectory
    """
```

### 4. Constraint Handling

#### A. Coverage Constraint
```python
# Constraint: average coverage ‚â• œÑ
coverage_constraint = tau - cov_k.mean()  # ‚â§ 0 when satisfied

# Dual update: Œª ‚Üê max(0, Œª + Œ∑_dual √ó violation)
lambda_cov = max(0.0, lambda_cov + eta_dual * coverage_constraint)
```

#### B. Fairness Constraint  
```python
# Constraint: per-group error ‚â§ Œ¥
fairness_violations = e_k - delta  # ‚â§ 0 when satisfied

# Dual update: ŒΩ_k ‚Üê max(0, ŒΩ_k + Œ∑_dual √ó violation_k)  
nu = torch.clamp(nu + eta_dual * fairness_violations, min=0.0)

# Adaptive Œ¥ adjustment
if adaptive_delta and current_avg_error > 0:
    delta = max(delta * 0.95, delta_multiplier * current_avg_error)
```

### 5. Primal-Dual Updates

#### Primal Update (Œ±, Œº)
```python
# Grid search over Œº candidates
for lam in lambda_grid:
    mu_candidate = torch.tensor([+lam/2, -lam/2])  # For K=2
    
    # Fixed-point updates for Œ± given Œº
    alpha_candidate = alpha.clone()
    for fp_iter in range(3):
        # Compute acceptance rates
        raw_margins = compute_raw_margin(eta_S1, alpha_candidate, mu_candidate, class_to_group)
        accepted = (raw_margins >= t)
        
        # Update Œ± based on acceptance rates
        for k in range(K):
            group_mask = (class_to_group[y_S1] == k)
            acceptance_rate = (accepted & group_mask).sum() / group_mask.sum()
            alpha_new[k] = acceptance_rate + regularization
        
        # EMA smoothing
        alpha_candidate = 0.7 * alpha_candidate + 0.3 * alpha_new
        alpha_candidate = project_alpha(alpha_candidate)
    
    # Evaluate Lagrangian objective
    e_k, cov_k, _, _ = compute_group_metrics(eta_S2, y_S2, alpha_candidate, mu_candidate, t)
    
    lagrangian = e_k.mean()  # Primal objective
    if outer_iter >= warmup_iters:
        lagrangian += lambda_cov * (tau - cov_k.mean())  # Coverage penalty
        lagrangian += (nu * torch.clamp(e_k - delta, min=0)).sum()  # Fairness penalty
    
    # Track best candidate
    if lagrangian < best_lagrangian:
        best_alpha, best_mu = alpha_candidate, mu_candidate
```

#### Dual Update (Œª, ŒΩ)
```python
# Coverage multiplier update
lambda_cov = max(0.0, lambda_cov + eta_dual * coverage_violation)

# Per-group fairness multiplier update  
nu = torch.clamp(nu + eta_dual * fairness_violations, min=0.0)
```

### 6. Output Analysis

```python
# Final Results Example:
Œ±* = [1.0145, 0.9823]     # Balanced scaling between groups
Œº* = [0.0234, -0.0234]    # Small bias correction
t* = 0.4567               # Threshold for œÑ=0.65 coverage

# Metrics achieved:
balanced_error = 0.0987   # Primary objective
worst_error = 0.1234     # Worst group performance  
coverage = 0.653         # Satisfies œÑ ‚â• 0.65
per_group_errors = [0.0876, 0.1098]  # Both ‚â§ Œ¥
per_group_coverage = [0.645, 0.661]  # Balanced coverage

# Constraint satisfaction:
coverage_satisfied = True    # 0.653 ‚â• 0.65 ‚úì
fairness_satisfied = True   # All e_k ‚â§ Œ¥ ‚úì
```

---

## üîÑ So s√°nh hai thu·∫≠t to√°n

| Aspect | GSE-Balanced | GSE-Constrained |
|--------|--------------|-----------------|
| **Objective** | Minimize balanced/worst error | Minimize error subject to constraints |
| **Constraints** | None (unconstrained) | Coverage ‚â• œÑ, Error ‚â§ Œ¥ |  
| **Optimization** | Direct minimization | Lagrangian dual method |
| **Complexity** | Simpler, faster | More complex, principled |
| **Use case** | General optimization | Fairness-aware deployment |
| **Parameters** | (Œ±, Œº, t) | (Œ±, Œº, t) + (Œª, ŒΩ) |

---

## üìä Evaluation v√† Metrics

### 1. Performance Metrics
```python
def evaluate_final_performance(eta_S2, y_S2, alpha_star, mu_star, t_star, class_to_group, K):
    """
    ƒê√°nh gi√° hi·ªáu su·∫•t cu·ªëi c√πng tr√™n validation set.
    
    Computed Metrics:
    - Balanced Error: (1/K) Œ£ e_k  
    - Worst-case Error: max_k e_k
    - Coverage: fraction of samples accepted
    - Per-group Error: e_k for each group k
    - Per-group Coverage: cov_k for each group k
    - Accuracy on accepted: correct predictions / accepted predictions
    """
```

### 2. Convergence Analysis
```python
# Tracking optimization history:
history = {
    'balanced_error': [...],     # Balanced error per iteration
    'worst_error': [...],        # Worst error per iteration  
    'coverage': [...],           # Coverage per iteration
    'lagrangian': [...],         # Objective per iteration (constrained only)
    'lambda_cov': [...],         # Coverage multiplier (constrained only)
    'nu': [...],                # Fairness multipliers (constrained only)
    'alpha': [...],             # Alpha evolution
    'mu': [...]                 # Mu evolution
}
```

---

## üöÄ Usage Examples

### 1. Ch·∫°y GSE-Balanced Plugin
```bash
cd c:\Users\Admin\Documents\GitHub\AR-GSE
python -m src.train.gse_balanced_plugin

# Output:
# ‚úÖ Loaded S1 (tuneV): 10 batches  
# ‚úÖ Loaded S2 (val_lt): 10 batches
# ‚úÖ Groups: 34 head classes, 66 tail classes
# ‚úÖ Loaded pre-trained gating
# ‚úÖ Cached Œ∑ÃÉ_S1: [2500, 100], y_S1: [2500]
# ‚úÖ Cached Œ∑ÃÉ_S2: [2500, 100], y_S2: [2500]
# 
# === GSE Balanced Plugin ===
# Objective: balanced, T=50, EMA=0.70
# [1 ] bal=0.2134, worst=0.3456, obj=0.2134
# [2 ] bal=0.1987, worst=0.3201, obj=0.1987  
# ...
# [23] bal=0.0987, worst=0.1234, obj=0.0987
# Early stopping at iteration 23
# 
# Œ±* = [1.0145, 0.9823]
# Œº* = [0.0234, -0.0234]  
# t* = 0.4567
# üíæ Saved checkpoint to ./checkpoints/argse_balance/cifar100_lt_if100/gse_balanced_plugin.ckpt
```

### 2. Ch·∫°y GSE-Constrained Plugin
```bash 
python -m src.train.gse_constrained_plugin

# Output:
# === GSE Constrained Plugin ===
# Coverage constraint: œÑ ‚â• 0.65
# Outer iterations: 50
# Initial Œ¥ = 0.267 (1.3√ó avg error)
# 
# [1 ] bal=0.2134, worst=0.3456, cov=0.580, L=0.2134
# [2 ] bal=0.1987, worst=0.3201, cov=0.623, L=0.1987
# ...  
# [15] bal=0.0987, worst=0.1234, cov=0.653, L=0.0956, Œª=0.234, ŒΩ_max=0.123
# Early stopping at iteration 15
#
# ‚úÖ Final Results:
# Œ±* = [1.0145, 0.9823]
# Œº* = [0.0234, -0.0234]
# Coverage = 0.653 (‚â• 0.65 ‚úì)
# Per-group errors: [0.0876, 0.1098] (all ‚â§ 0.267 ‚úì)
# üíæ Saved checkpoint + plots
```

---

## üîß Configuration tuning

### 1. Key hyperparameters

#### GSE-Balanced:
- `T`: S·ªë iterations (50 th∆∞·ªùng ƒë·ªß)
- `ema_alpha`: EMA coefficient (0.7 cho stability)  
- `lambda_grid`: Range cho Œº optimization ([-2, 2])
- `alpha_min/max`: Constraints cho Œ± ([0.75, 1.35])
- `objective`: 'balanced' vs 'worst' vs 'hybrid'

#### GSE-Constrained:
- `tau`: Coverage constraint (0.65 = 65% minimum)
- `delta_multiplier`: Fairness constraint (1.3√ó avg error) 
- `eta_dual`: Dual step size (0.05 cho stability)
- `warmup_iters`: Iterations tr∆∞·ªõc khi √°p d·ª•ng constraints (10)

### 2. Troubleshooting

#### Common issues:
1. **Convergence problems**: Gi·∫£m eta_dual/eta_primal
2. **Unstable Œ± updates**: TƒÉng ema_alpha 
3. **Coverage too low**: Gi·∫£m tau ho·∫∑c tƒÉng delta_multiplier
4. **Poor fairness**: ƒêi·ªÅu ch·ªânh lambda_grid range

---

## üìÅ File Structure

```
checkpoints/argse_balance/cifar100_lt_if100/
‚îú‚îÄ‚îÄ gse_balanced_plugin.ckpt      # Balanced plugin results
‚îî‚îÄ‚îÄ optimization_history.png      # Training curves

checkpoints/argse_constrained_plugin/cifar100_lt_if100/  
‚îú‚îÄ‚îÄ gse_constrained_plugin.ckpt   # Constrained plugin results
‚îî‚îÄ‚îÄ constrained_optimization_history.png  # Constraint satisfaction curves

outputs/logits/cifar100_lt_if100/  # Input expert logits
‚îú‚îÄ‚îÄ ce_baseline/
‚îú‚îÄ‚îÄ logitadjust_baseline/  
‚îî‚îÄ‚îÄ balsoftmax_baseline/
```

---

## üéØ Summary

**GSE Ensemble Training** l√† giai ƒëo·∫°n cu·ªëi c√πng k·∫øt h·ª£p t·∫•t c·∫£ components c·ªßa AR-GSE:

1. **Input**: Expert logits + Pre-trained gating + Data splits
2. **Process**: Plugin optimization v·ªõi primal-dual methods  
3. **Output**: Optimal ensemble parameters (Œ±*, Œº*, t*)
4. **Result**: High-performance selective prediction system

Hai thu·∫≠t to√°n plugin cung c·∫•p flexibility:
- **Balanced**: ƒê∆°n gi·∫£n, t·ªëi ∆∞u performance tr·ª±c ti·∫øp
- **Constrained**: Ph·ª©c t·∫°p h∆°n nh∆∞ng ƒë·∫£m b·∫£o fairness constraints

K·∫øt qu·∫£ cu·ªëi c√πng l√† m·ªôt AR-GSE model ho√†n ch·ªânh c√≥ th·ªÉ deploy cho selective prediction tr√™n CIFAR-100-LT v·ªõi hi·ªáu su·∫•t cao v√† c√¥ng b·∫±ng gi·ªØa c√°c nh√≥m head/tail classes.