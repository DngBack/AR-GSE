"""
Verify plugin checkpoint and analyze parameters
"""
import torch
import json
from pathlib import Path

# Load checkpoint
ckpt_path = "checkpoints/argse_worst_eg_improved/cifar100_lt_if100/gse_balanced_plugin.ckpt"
print(f"üìÇ Loading checkpoint: {ckpt_path}")
ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)

print("\n" + "="*60)
print("CHECKPOINT CONTENTS")
print("="*60)
for key in ckpt.keys():
    if isinstance(ckpt[key], torch.Tensor):
        print(f"{key:30s}: shape={list(ckpt[key].shape)}, dtype={ckpt[key].dtype}")
    else:
        print(f"{key:30s}: type={type(ckpt[key])}")

print("\n" + "="*60)
print("PLUGIN PARAMETERS")
print("="*60)

# Alpha (acceptance parameters)
alpha = ckpt['alpha']
print(f"Œ± (alpha): {alpha.tolist()}")
print(f"  ‚Üí Group 0 (head): Œ±={alpha[0]:.4f}")
print(f"  ‚Üí Group 1 (tail): Œ±={alpha[1]:.4f}")
print(f"  ‚Üí Interpretation: Higher Œ± means more selective (higher threshold)")

# Mu (threshold adjustments)
mu = ckpt['mu']
print(f"\nŒº (mu): {mu.tolist()}")
print(f"  ‚Üí Group 0 (head): Œº={mu[0]:.4f}")
print(f"  ‚Üí Group 1 (tail): Œº={mu[1]:.4f}")
print(f"  ‚Üí Interpretation: Positive Œº increases threshold (more selective)")

# Class to group mapping
class_to_group = ckpt['class_to_group']
num_head = (class_to_group == 0).sum().item()
num_tail = (class_to_group == 1).sum().item()
print(f"\nClass to Group mapping:")
print(f"  ‚Üí Group 0 (head): {num_head} classes")
print(f"  ‚Üí Group 1 (tail): {num_tail} classes")

# Best score
if 'best_score' in ckpt:
    print(f"\nBest validation score: {ckpt['best_score']:.4f}")
    print(f"  ‚Üí This is the error achieved during plugin training")

# Threshold
if 'threshold' in ckpt:
    threshold = ckpt['threshold']
    if isinstance(threshold, list):
        print(f"\nThreshold c*: {threshold}")
    else:
        print(f"\nThreshold c*: {threshold:.4f}")
    print(f"  ‚Üí This is the optimal rejection cost found during training")
if 't_group' in ckpt:
    t_group = ckpt['t_group']
    print(f"\nPer-group thresholds: {t_group}")
    print(f"  ‚Üí Group 0 (head): {t_group[0] if isinstance(t_group, list) else t_group}")
    print(f"  ‚Üí Group 1 (tail): {t_group[1] if isinstance(t_group, list) and len(t_group) > 1 else 'N/A'}")

print("\n" + "="*60)
print("PARAMETER ANALYSIS")
print("="*60)

# Check if parameters are identity (not optimized)
if torch.allclose(alpha, torch.ones(2)):
    print("‚ö†Ô∏è  WARNING: Œ± = [1, 1] ‚Üí Identity, not optimized!")
else:
    print("‚úÖ Œ± has been optimized (not identity)")

if torch.allclose(mu, torch.zeros(2)):
    print("‚ö†Ô∏è  WARNING: Œº = [0, 0] ‚Üí No adjustment!")
else:
    print("‚úÖ Œº has been optimized")
    if mu[0] < 0 and mu[1] > 0:
        print("   ‚Üí Head group: more lenient (Œº < 0)")
        print("   ‚Üí Tail group: more selective (Œº > 0)")
        print("   ‚Üí This is EXPECTED for long-tail (protect tail from errors)")

# Compute effective threshold difference
# For sample in group k: threshold = Œ£_y [(1/Œ±_k - Œº_k) * Œ∑_y]
head_factor = 1.0/alpha[0].item() - mu[0].item()
tail_factor = 1.0/alpha[1].item() - mu[1].item()
print(f"\nEffective threshold factors:")
print(f"  ‚Üí Head (1/Œ± - Œº): {head_factor:.4f}")
print(f"  ‚Üí Tail (1/Œ± - Œº): {tail_factor:.4f}")
print(f"  ‚Üí Difference: {tail_factor - head_factor:.4f}")
if tail_factor > head_factor:
    print("  ‚Üí Tail has HIGHER threshold ‚Üí More selective (reject more)")
    print("  ‚Üí This may cause low coverage for tail group!")

print("\n" + "="*60)
print("POTENTIAL ISSUES")
print("="*60)

issues = []

# Check if Œ± = 1 (no optimization)
if torch.allclose(alpha, torch.ones(2)):
    issues.append("Œ± not optimized (identity)")

# Check if Œº causes extreme selectivity
if abs(mu[0].item()) > 1.0 or abs(mu[1].item()) > 1.0:
    issues.append(f"Large Œº values (|Œº| > 1.0): may cause extreme behavior")

# Check if tail is too selective
if tail_factor > head_factor + 0.5:
    issues.append("Tail group significantly more selective than head")
    issues.append("‚Üí May explain why worst-group has low coverage/high risk")

if len(issues) == 0:
    print("‚úÖ No major issues detected")
else:
    for i, issue in enumerate(issues, 1):
        print(f"{i}. {issue}")

print("\n" + "="*60)
print("RECOMMENDATIONS")
print("="*60)

if torch.allclose(alpha, torch.ones(2)) and not torch.allclose(mu, torch.zeros(2)):
    print("1. Œ± = [1, 1] but Œº ‚â† [0, 0]")
    print("   ‚Üí Plugin training may have fixed Œ± and only optimized Œº")
    print("   ‚Üí This is OK if intended, but limits flexibility")

if tail_factor > head_factor + 0.5:
    print("2. Tail group is much more selective than head")
    print("   ‚Üí Try different plugin training with:")
    print("      - Balanced objective (instead of worst)")
    print("      - Lower target coverage")
    print("      - Different initialization")

print("\nüìù Next steps:")
print("1. Check if plugin training converged properly")
print("2. Compare with gating-only checkpoint (before plugin)")
print("3. Try re-training plugin with different hyperparameters")
